{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "import pandas as pd\n",
    "# from dhs_preprocessing_functions import *\n",
    "# from pandarallel import pandarallel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import xgboost as xgb\n",
    "from keras import regularizers\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "# Initialization\n",
    "# pandarallel.initialize()\n",
    "import sys\n",
    "sys.path.append('/home/myuser/prj/code/FoodSecurity/DHS/')\n",
    "print(sys.path)\n",
    "\n",
    "from dhs_modelling_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to dos:\n",
    "# 1. add a function to plot the feature importance\n",
    "# 2. add a function to plot the partial dependence plots\n",
    "# 3. implement train, val test in modeling function\n",
    "# 4. use r2 score as a loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"/mnt/datadisk/data/Projects/water/inputs/\"\n",
    "out_dir = input_dir + 'country_data_cross_val_results/'\n",
    "# min_version = 3\n",
    "overwrite_pqt = True\n",
    "dataset_type = 'HR'  #, 'BR', 'CR', 'HW', 'IR', 'KR', 'MR', 'PR', 'AR'] ??['AN', 'FC, 'FP', 'HW', 'PV', 'SC'] < 25 surveys\n",
    "urban_rural_all_mode = 'all' # 'all', 'U', 'R'\n",
    "group_by_col = 'adm2_gaul'\n",
    "# out_f = f\"{input_dir}5_grouped_df_V3_{dataset_type}_{group_by_col}_joined_with_ipc_{urban_rural_all_mode}.pkl\"\n",
    "\n",
    "# Scale options\n",
    "scale_numerical_data = False\n",
    "scale_all_data = True\n",
    "leave_out_encodings = False\n",
    "zero_one_scale_categorical = False\n",
    "scale_labels = True\n",
    "\n",
    "# More Options\n",
    "drop_agriculture = False\n",
    "use_pca = False\n",
    "drop_encodings = False\n",
    "load_best_model = True\n",
    "use_3_layer_nn = True \n",
    "neurons_first_layer = 256\n",
    "add_dropout = False\n",
    "regularizer_type = False\n",
    "regularizer_value = 0.05\n",
    "add_n = 'imputed_L2_0.5'\n",
    "use_imputations = False\n",
    "model_n = 'linear' # nn, linear or xgb\n",
    "fold_by = 'country' # country, year, survey or unconditional\n",
    "print_col = 'Meta; adm0_gaul'\n",
    "drop_perc = 20\n",
    "epochs = 100\n",
    "use_sustainbench = False\n",
    "test_mode = False\n",
    "use_fs_as_input = True\n",
    "test_function_to_use = 'mse' #correlation, rmse, r2, mse\n",
    "use_additional_subset = False\n",
    "\n",
    "# Combine these into a dict for results.csv\n",
    "options = {\n",
    "    'scale_numerical_data': scale_numerical_data,\n",
    "    'scale_all_data': scale_all_data,\n",
    "    'leave_out_encodings': leave_out_encodings,\n",
    "    'zero_one_scale_categorical': zero_one_scale_categorical,\n",
    "    'scale_labels': scale_labels,\n",
    "    'drop_agriculture': drop_agriculture,\n",
    "    'use_pca': use_pca,\n",
    "    'drop_encodings': drop_encodings,\n",
    "    'load_best_model': load_best_model,\n",
    "    'use_3_layer_nn': use_3_layer_nn,\n",
    "    'add_dropout': add_dropout,\n",
    "    'regularizer': regularizer_type,\n",
    "    'regularizer_value': regularizer_value,\n",
    "    'add_n': add_n,\n",
    "    'use_imputations': use_imputations,\n",
    "    'model_n': model_n,\n",
    "    'fold_by': fold_by,\n",
    "    'print_col': print_col,\n",
    "    'drop_perc': drop_perc,\n",
    "    'epochs': epochs,\n",
    "    'use_sustainbench': use_sustainbench,\n",
    "    'test_mode': test_mode,\n",
    "    'use_fs_as_input': use_fs_as_input,\n",
    "    'test_function_to_use': test_function_to_use,\n",
    "    'use_additional_subset': use_additional_subset,\n",
    "    'neurons_first_layer': neurons_first_layer,\n",
    "}\n",
    "\n",
    "#Other Options\n",
    "overwrite_results = False\n",
    "write_PDP_plots = False\n",
    "\n",
    "\n",
    "# in_f = f\"{input_dir}5_grouped_df_V3_{dataset_type}_{group_by_col}_joined_with_ipc_{urban_rural_all_mode}.pkl\"\n",
    "in_f = f\"{input_dir}GDP_CPI_weoapr_preprocessed.csv\"\n",
    "\n",
    "# in2 = f\"{input_dir}imputation_results_fold_by_survey_mask_categoricals_together_True_split_categoricals_False_drop_perc_19_MICE KNN_DF_imputed.csv\"\n",
    "\n",
    "# result_picture_f =\n",
    "result_csv_f = f\"{out_dir}results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if in_f[-4:] == '.pkl':\n",
    "    input_df = pd.read_pickle(in_f)\n",
    "else:\n",
    "    input_df = pd.read_csv(in_f)\n",
    "    \n",
    "for c in input_df.columns:\n",
    "    if 'year' in c or 'Year' in c:\n",
    "        print(c)\n",
    "\n",
    "# if urban_rural_all_mode == 'U':\n",
    "#     drop_agriculture = True\n",
    "\n",
    "drop_ds1, retrieve_ds = [], []\n",
    "if drop_encodings:\n",
    "    drop_ds1 = ['Meta one-hot encoding', 'Meta frequency encoding']\n",
    "if use_sustainbench:\n",
    "    retrieve_ds = ['DHS Sustainbench', 'FS']\n",
    "df = final_ds_droping_cols(input_df, drop_meta=False, drop_food_help=True, drop_perc=drop_perc,\n",
    "                           retain_month=False, drop_highly_correlated_cols=False, drop_region=True, \n",
    "                 drop_data_sets=drop_ds1, \n",
    "                 use_NAN_amount_and_replace_NANs_in_categorical=False, drop_agricultural_cols=drop_agriculture, \n",
    "                 drop_below_version=False, numerical_data=['mean'], retain_adm=False, retrieve_ds=retrieve_ds, \n",
    "                 retain_GEID_init=True, verbose=0)\n",
    "\n",
    "# drop_cols = [c for c in df.columns if 'mean perc. change' in c and not '05' in c]\n",
    "# df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# for col in df.columns:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_mask = False\n",
    "if use_imputations:\n",
    "    df_imputed = pd.read_csv(in2).drop(columns=['Unnamed: 0'])\n",
    "    use_cols = [c for c in df_imputed.columns if c in df.columns]\n",
    "    # Create a mask of missing values\n",
    "    missing_mask = df[use_cols].isna()\n",
    "    df = pd.concat([df.drop(columns=use_cols), df_imputed[use_cols]], axis=1)\n",
    "else:\n",
    "    missing_mask = df.isna()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RMSE\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "\n",
    "def r_squared_loss(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    r2 = 1 - SS_res/(SS_tot + K.epsilon())\n",
    "    return -r2\n",
    "\n",
    "def negative_correlation(y_true, y_pred):\n",
    "    y_true_centered = y_true - K.mean(y_true)\n",
    "    y_pred_centered = y_pred - K.mean(y_pred)\n",
    "    corr_num = K.sum(y_true_centered * y_pred_centered)\n",
    "    corr_den = K.sqrt(K.sum(K.square(y_true_centered)) * K.sum(K.square(y_pred_centered)))\n",
    "    corr_coef = corr_num / (corr_den + K.epsilon())\n",
    "    return -corr_coef\n",
    "\n",
    "if test_function_to_use == 'correlation':\n",
    "    loss_function = negative_correlation\n",
    "elif test_function_to_use == 'rmse':\n",
    "    loss_function = root_mean_squared_error\n",
    "elif test_function_to_use == 'r2':\n",
    "    loss_function = r_squared_loss\n",
    "elif test_function_to_use == 'mse':\n",
    "    loss_function = 'mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [c for c in df.columns if 'FS; IPC' in c]\n",
    "if use_fs_as_input:\n",
    "    label_cols = [c for c in label_cols if 'FS; IPC: -1-0y: mean' not in c]\n",
    "\n",
    "input_df2 = df.drop(columns=label_cols)\n",
    "\n",
    "if use_additional_subset:\n",
    "    remain_col = [\n",
    "                'Meta; adm0_gaul', \n",
    "                'gdp_cpi_weoapr; gross domestic product, constant prices: mean perc. change 05 years',\n",
    "                'Meta; weo country code',\n",
    "                #   'gdp_cpi_weoapr; gross domestic product based on purchasing-power-parity (ppp) share of world total - percent: mean perc. change 05 years',\n",
    "                #   'gdp_cpi_weoapr; gross domestic product, deflator - index: mean perc. change 05 years',\n",
    "                #   'gdp_cpi_weoapr; inflation, average consumer prices: mean perc. change 05 years',\n",
    "                  'gdp_cpi_weoapr; gross domestic product, current prices - purchasing power parity; international dollars - billions: mean perc. change 05 years'\n",
    "                #   'FS; IPC: -1-0y: mean',\n",
    "                  \n",
    "                  ]\n",
    "    input_df2 = input_df2.drop(columns=[c for c in input_df2.columns if c not in remain_col])\n",
    "\n",
    "label_cols = [c for c in label_cols if 'FS; IPC: -1-0y' not in c]\n",
    "\n",
    "num_cols = [c for c in input_df2.columns if 'DHS Num;' in c or c == 'Meta; Year' or 'FS; IPC' in c]\n",
    "\n",
    "##Corr Matrix\n",
    "# corr_mtrx_df = pd.concat([input_df2, df[label_cols[0]]], axis=1)\n",
    "# create_correlation_matrix(corr_mtrx_df.select_dtypes(include=[np.number]), f\"{out_dir}Correlation_matrix.png\")\n",
    "\n",
    "print('label_cols', label_cols)\n",
    "print('num_cols', num_cols)\n",
    "print('input_df2', input_df2.columns)\n",
    "# options['in cols'] = input_df2.columns\n",
    "scaler_l = []\n",
    "history_l = []\n",
    "y_l = []\n",
    "y_pred_l = []\n",
    "model_l = []\n",
    "amount_inputs = []\n",
    "amount_cols = []\n",
    "\n",
    "if os.path.exists(result_csv_f) and not overwrite_results:\n",
    "    final_result_df = pd.read_csv(result_csv_f)\n",
    "else:\n",
    "    final_result_df = pd.DataFrame()\n",
    "\n",
    "regularizer = None\n",
    "if regularizer_type == 'l2':\n",
    "    regularizer = regularizers.l2(regularizer_value)\n",
    "elif regularizer_type == 'l1':\n",
    "    regularizer = regularizers.l1(regularizer_value)\n",
    "\n",
    "results_d = defaultdict(list)\n",
    "results_f_d = defaultdict(list)\n",
    "for nr, col in enumerate(label_cols[0:1]):\n",
    "    all_results_dfs = []\n",
    "    all_val_results_dfs = []\n",
    "    print(nr, col, len(df[col]))\n",
    "    options['label_col'] = col\n",
    "    labels = df[col]\n",
    "    labels = labels.dropna()\n",
    "    # labels = labels[labels < 4]\n",
    "    labels_ind = labels.index\n",
    "    scaler = False\n",
    "    if scale_labels:\n",
    "        # scale the labels\n",
    "        scaler = StandardScaler()\n",
    "        labels = scaler.fit_transform(labels.values.reshape(-1, 1))\n",
    "        labels = pd.Series(labels.flatten(), index=labels_ind)\n",
    "    input_df = input_df2.loc[labels.index]\n",
    "    \n",
    "    # if model_n == 'nn':\n",
    "    #drop NaNs\n",
    "    input_df = input_df.dropna(axis=0)\n",
    "    input_ind = input_df.index\n",
    "    labels = labels.loc[input_ind]\n",
    "\n",
    "    #not dropping NaNs, since model can work like that\n",
    "    print('lens', len(labels), len(input_df))\n",
    "    print('cols', len(input_df.columns))\n",
    "    for col in input_df.columns:\n",
    "        print(col, input_df[col].dtype)\n",
    "    if len(input_df) < 10:\n",
    "        continue\n",
    "    \n",
    "    amount_cols.append(len(input_df.columns))\n",
    "    assert(labels.index.equals(input_df.index))\n",
    "    amount_inputs.append(len(input_df))\n",
    "    scaler_l.append(scaler)\n",
    "\n",
    "    scaler2 = StandardScaler()    \n",
    "    cols_w_numerical_data = input_df.select_dtypes([np.number]).columns.to_list()\n",
    "    if scale_all_data:\n",
    "        drop_cols = ['Meta; adm0_gaul', 'Meta; GEID_init']\n",
    "        if leave_out_encodings:\n",
    "            cols = [c for c in cols_w_numerical_data if not any([True for drop_c in ['Meta one-hot encoding;', 'Meta frequency encoding;']\n",
    "                                                            + drop_cols if drop_c in c])]\n",
    "            input_df[cols] = scaler2.fit_transform(input_df[cols])\n",
    "        else:\n",
    "            cols = [c for c in cols_w_numerical_data if c not in drop_cols]\n",
    "            input_df[cols] = scaler2.fit_transform(input_df[cols])\n",
    "    else:\n",
    "        if scale_numerical_data:\n",
    "            input_df[num_cols] = scaler2.fit_transform(input_df[num_cols])\n",
    "        \n",
    "        if zero_one_scale_categorical:\n",
    "            scaler3 = MinMaxScaler()\n",
    "            cols = [c for c in input_df.columns if 'DHS Cat;' in c]\n",
    "            input_df[cols] = scaler3.fit_transform(input_df[cols])\n",
    "    \n",
    "    # input_df.to_csv(f\"{input_dir}Prefiltered_normalized_grouped_df_V3_{dataset_type}_{group_by_col}_{urban_rural_all_mode}_m_{col}.csv\")\n",
    "       \n",
    "    fold_gen = fold_generator_3_independent_indices(input_df, fold_by, n_splits=5)\n",
    "    # Iterate over each fold\n",
    "    for fold, (train_index, val_index, test_index) in enumerate(fold_gen):\n",
    "        assert(labels.index.equals(input_df.index))\n",
    "\n",
    "        X_train, X_val, X_test = input_df.loc[train_index], input_df.loc[val_index], input_df.loc[test_index]\n",
    "        y_train, y_val, y_test = labels.loc[train_index], labels.loc[val_index], labels.loc[test_index]          \n",
    "        y_test_index = y_test.index\n",
    "        print(fold)\n",
    "        try:\n",
    "            print('train set', X_train[print_col].unique())\n",
    "            print('val set', X_val[print_col].unique())\n",
    "            print('test set', X_test[print_col].unique())\n",
    "            print('Shapes', X_train.shape, X_val.shape, X_test.shape)\n",
    "        except KeyError:\n",
    "            print('Shapes', X_train.shape, X_val.shape, X_test.shape)\n",
    "        # print(X_train)\n",
    "        # print(y_train)\n",
    "        # drop cols\n",
    "        \n",
    "        drop_cols = ['Meta; adm0_gaul', 'Meta; GEID_init']\n",
    "        drop_cols = [c for c in drop_cols if c in X_train.columns]\n",
    "        if 'Meta; rounded year' in X_train.columns:\n",
    "            drop_cols.append('Meta; rounded year')\n",
    "\n",
    "        numerical_cols = X_train.select_dtypes(include=[np.number]).columns\n",
    "        X_train = X_train[numerical_cols].drop(columns=drop_cols, errors='ignore')\n",
    "        X_val = X_val[numerical_cols].drop(columns=drop_cols, errors='ignore')\n",
    "        X_test = X_test[numerical_cols].drop(columns=drop_cols, errors='ignore')\n",
    "        \n",
    "        n_components = False\n",
    "        if use_pca:\n",
    "            # create PCA\n",
    "            n_components = use_pca\n",
    "            if use_pca == True:\n",
    "                n_components = 0.98\n",
    "            pca = PCA(n_components=n_components)\n",
    "            pca = pca.fit(X_train)\n",
    "            X_test = pca.transform(X_test)\n",
    "            X_val = pca.transform(X_val)\n",
    "            X_train = pca.transform(X_train)\n",
    "            # Calculate the cumulative explained variance\n",
    "            cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "            print('PCA', X_train.shape, X_val.shape, X_test.shape)\n",
    "            print('Cumulative explained variance', cumulative_explained_variance)\n",
    "            print(len(cumulative_explained_variance))\n",
    "            \n",
    "        all_dfs = []\n",
    "        for name, X, y in zip(['train', 'val','test'], [X_train, X_val, X_test], [y_train, y_val, y_test]):\n",
    "            ##Corr Matrix\n",
    "            corr_mtrx_df = pd.concat([X, y], axis=1)\n",
    "            print('hi', fold, name)\n",
    "            # print(y_t)\n",
    "            create_correlation_matrix(corr_mtrx_df, f\"{out_dir}Correlation_matrix_fold{fold}_{name}.png\")\n",
    "            all_dfs.append(corr_mtrx_df)\n",
    "\n",
    "        corr_mtrx_all = pd.concat(all_dfs)\n",
    "        print(fold, 'all')\n",
    "        create_correlation_matrix(corr_mtrx_all, f\"{out_dir}Correlation_matrix_fold{fold}_all.png\")\n",
    "        \n",
    "        history = False\n",
    "        if model_n == 'xgb':\n",
    "            # Initialize the XGBoost model\n",
    "            model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "        elif model_n == 'linear':\n",
    "            # Initialize the Linear Regression model\n",
    "            model = LinearRegression()\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "        elif model_n == 'nn':\n",
    "            if use_3_layer_nn:\n",
    "                # Define the model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(neurons_first_layer, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=regularizer))\n",
    "                model.add(Dense(64, activation='relu', kernel_regularizer=regularizer))\n",
    "                # model.add(Dense(32, activation='relu', kernel_regularizer=regularizer))\n",
    "                if add_dropout:\n",
    "                    model.add(Dropout(add_dropout))\n",
    "                model.add(Dense(1, activation='linear'))\n",
    "            else:\n",
    "                # Define the model\n",
    "                model = Sequential()\n",
    "                model.add(Dense(neurons_first_layer, activation='relu', input_shape=(X_train.shape[1],), kernel_regularizer=regularizer))\n",
    "                if add_dropout:\n",
    "                    model.add(Dropout(add_dropout))\n",
    "                model.add(Dense(1, activation='linear'))\n",
    "                \n",
    "            # Compile the model\n",
    "            model.compile(optimizer=Adam(learning_rate=0.001), loss=loss_function, metrics=[loss_function])\n",
    "\n",
    "            checkpoint_n = 'best_model.keras'\n",
    "            # Define the ModelCheckpoint callback\n",
    "            checkpoint = ModelCheckpoint(checkpoint_n, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min')\n",
    "            \n",
    "            # Train the model\n",
    "            history = model.fit(X_train, y_train, epochs=epochs, batch_size=32, validation_data=(X_val, y_val), callbacks=[checkpoint, early_stopping])\n",
    "            history_l.append(history)\n",
    "                       \n",
    "            if load_best_model:\n",
    "                # Load the best model\n",
    "                print('loading best model')\n",
    "                model.load_weights(checkpoint_n)\n",
    "            \n",
    "            # Evaluate the model on the test set\n",
    "            test_loss, test_rmse = model.evaluate(X_val, y_val)\n",
    "            print(f'Val Loss: {test_loss}')\n",
    "            print(f'Val RMSE: {test_rmse}')\n",
    "            \n",
    "            test_loss, test_rmse = model.evaluate(X_test, y_test)\n",
    "            print(f'Test Loss: {test_loss}')\n",
    "            print(f'Test RMSE: {test_rmse}')\n",
    "        \n",
    "            create_history_figures(history, f\"{out_dir}\", fold)\n",
    "        \n",
    "        for nrx, (X, y) in enumerate(zip([X_val, X_test], [y_val, y_test])):\n",
    "            # Predict on the test set\n",
    "            y_pred = model.predict(X)\n",
    "            y_index = y.index\n",
    "            y = np.array(y)\n",
    "            if y.ndim == 1:\n",
    "                y = y.reshape(-1, 1)\n",
    "            if y_pred.ndim == 1:\n",
    "                y_pred = y_pred.reshape(-1, 1)\n",
    "            \n",
    "            # Inverse transform the predictions\n",
    "            if scale_labels:\n",
    "                y_pred = scaler.inverse_transform(y_pred)\n",
    "                y = scaler.inverse_transform(y)\n",
    "                    \n",
    "            assert len(y) == len(y_pred)\n",
    "            # Create a DataFrame with 'Actual' and 'Prediction' columns\n",
    "            results_df = pd.DataFrame({\n",
    "                'Actual': y.flatten(),\n",
    "                'Prediction': y_pred.flatten()\n",
    "            }, index=y_index)\n",
    "            \n",
    "            # create_scatterplot(results_df, f\"{out_dir}Scatterplot_{col}_fold{fold}.png\")\n",
    "            if nrx == 1:\n",
    "                all_results_dfs.append(results_df)\n",
    "            if nrx == 0:\n",
    "                all_val_results_dfs.append(results_df)\n",
    "        \n",
    "        if write_PDP_plots:\n",
    "            # Create partial dependence plots\n",
    "            create_PDP_plots(X_test, model, f\"{input_dir}DHS_PDP_NN/\", fold)\n",
    "        \n",
    "        if test_mode:\n",
    "            print(f\"{test_mode}: breaking\")\n",
    "            break\n",
    "        \n",
    "    all_result_df = pd.concat(all_results_dfs)\n",
    "    all_val_result_df = pd.concat(all_val_results_dfs)\n",
    "    print(all_result_df.shape)\n",
    "    print(all_val_result_df.shape)\n",
    "    print(labels.shape)\n",
    "\n",
    "    missing_mask = missing_mask.loc[all_result_df.index]\n",
    "    final_p_res_df = metrices_weighted_available_data(all_result_df, missing_mask, drop_perc=drop_perc)\n",
    "\n",
    "    # Add options to every row of final_p_res_df\n",
    "    final_p_res_df = final_p_res_df.assign(**options)\n",
    "    # print('new', final_p_res_df)\n",
    "    # print('old', final_result_df)\n",
    "    create_scatterplot(all_val_result_df, f\"{out_dir}Scatterplot_Val_{col}.png\")\n",
    "    create_scatterplot(all_result_df, f\"{out_dir}Scatterplot_Test_{col}.png\")\n",
    "\n",
    "    # Add results_d to result_df\n",
    "    final_result_df = pd.concat([final_p_res_df, final_result_df])\n",
    "    \n",
    "final_result_df.to_csv(result_csv_f, index=False)\n",
    "final_p_res_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_p_res_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
