{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91a36d1427f1fb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:04:17.734460242Z",
     "start_time": "2023-11-15T15:03:54.955100134Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyreadstat\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use dask/ray\n",
    "# import modin.pandas as pd\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "from functools import partial\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed, ThreadPoolExecutor\n",
    "from icecream import ic\n",
    "#import memory_profiler\n",
    "from memory_profiler import profile\n",
    "%load_ext memory_profiler\n",
    "\n",
    "#own imports\n",
    "from dhs_pca_functions import *\n",
    "from dhs_gpt import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1a4c54ba791942",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:04:17.762847915Z",
     "start_time": "2023-11-15T15:03:55.514021745Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "folder_path = \"/mnt/datadisk/data/surveys/DHS_final_raw_data/\"\n",
    "work_dir = \"/mnt/datadisk/data/Projects/water/pickles/\"\n",
    "input_dir = \"/mnt/datadisk/data/Projects/water/inputs/\"\n",
    "min_version = 8\n",
    "max_version = False\n",
    "overwrite_pqt = False\n",
    "parquet = False\n",
    "use_dask = False\n",
    "drop_threshold = False\n",
    "drop_col_counter = False\n",
    "cols_to_load_simultaneously = 15\n",
    "dataset_type = 'BR'  #, 'BR', 'CR', 'HW', 'IR', 'KR', 'MR', 'PR', 'AR'] ??['AN', 'FC, 'FP', 'HW', 'PV', 'SC'] < 26 surveys\n",
    "add_str = 'pd'\n",
    "columns = []\n",
    "verbose = False\n",
    "available_since_v = 7\n",
    "pqt_file = f\"{work_dir}New_DHS_{dataset_type}_data_V{min_version}_cols_{drop_col_counter}_{add_str}_new\"\n",
    "if parquet:\n",
    "    pqt_file += '.pqt'\n",
    "else:\n",
    "    pqt_file += '.pkl'\n",
    "further_pqt_files = f\"{work_dir}DHS_{dataset_type}_data_V{min_version}_cols_{drop_col_counter}_{add_str}_\"\n",
    "excl_countries = [] #['EG', 'ZA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ec0e6a4b1d28fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:04:17.796881579Z",
     "start_time": "2023-11-15T15:03:55.520412453Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/datadisk/data/surveys/DHS_final_raw_data/BFBR81SV/BFBR81FL.SAV'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load column names from all .sav files\n",
    "file_paths = get_dhs_files(folder_path, dataset_type, min_version, excl_countries=excl_countries, max_version=max_version)\n",
    "file_paths = sorted(file_paths)\n",
    "ic(len(file_paths))\n",
    "ic(file_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9ece15eddfe71e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:04:17.804680525Z",
     "start_time": "2023-11-15T15:03:55.571497224Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "further_pqt_file = further_pqt_files + 'cols.pkl'\n",
    "if not os.path.exists(further_pqt_file) or overwrite_pqt:\n",
    "    common_column_labels, common_column_names, missing_label_details, uncommon_column_names, uncommon_column_labels, all_col_names, all_col_labels = get_common_columns_new(file_paths)\n",
    "    with open(further_pqt_file, 'wb') as f:\n",
    "        pickle.dump([common_column_labels, common_column_names, missing_label_details, uncommon_column_names, uncommon_column_labels, all_col_names, all_col_labels], f)\n",
    "else:\n",
    "    with open(further_pqt_file, 'rb') as f:\n",
    "        common_column_labels, common_column_names, missing_label_details, uncommon_column_names, uncommon_column_labels, all_col_names, all_col_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95922c4b248d09af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:04:17.825961886Z",
     "start_time": "2023-11-15T15:03:55.619778437Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/211 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/211 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non unique cluster + hh nr and non unique case identification - skipping survey",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nconcurrent.futures.process._RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 239, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 198, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 198, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/home/sven/pycharm/FoodSecurity/DHS/dhs_pca_functions.py\", line 429, in decode_data\n    raise ValueError('non unique cluster + hh nr and non unique case identification - skipping survey')\nValueError: non unique cluster + hh nr and non unique case identification - skipping survey\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 239, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 198, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 198, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/home/sven/pycharm/FoodSecurity/DHS/dhs_pca_functions.py\", line 1254, in summary_df_loading_wrapper\n    df = loading_wrapper(start_nr, cols_to_load_simultaneously, file_paths, dataset_type, folder_path, all_col_labels, create_GEID=create_GEID,\n  File \"/home/sven/pycharm/FoodSecurity/DHS/dhs_pca_functions.py\", line 1245, in loading_wrapper\n    all_data, meta_data = load_data(file_paths, dataset_type, folder_path, col_labels=col_labels, create_GEID=create_GEID, create_DHSID=create_DHSID,\n  File \"/home/sven/pycharm/FoodSecurity/DHS/dhs_pca_functions.py\", line 544, in load_data\n    results = list(executor.map(partial_decode_data, files))\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 484, in _chain_from_iterable_of_lists\n    for element in iterable:\n  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 619, in result_iterator\n    yield fs.pop().result()\n  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 444, in result\n    return self.__get_result()\n  File \"/usr/lib/python3.8/concurrent/futures/_base.py\", line 389, in __get_result\n    raise self._exception\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 239, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 198, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/usr/lib/python3.8/concurrent/futures/process.py\", line 198, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/home/sven/pycharm/FoodSecurity/DHS/dhs_pca_functions.py\", line 429, in decode_data\n    raise ValueError('non unique cluster + hh nr and non unique case identification - skipping survey')\nValueError: non unique cluster + hh nr and non unique case identification - skipping survey\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#overwrite_pqt = True\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(summary_f) \u001b[38;5;129;01mor\u001b[39;00m overwrite_pqt:\n\u001b[0;32m----> 4\u001b[0m     summary_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_summary_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_col_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols_to_load_simultaneously\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_GEID\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_DHSID\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavailable_since_v\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mavailable_since_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     summary_df\u001b[38;5;241m.\u001b[39mto_csv(summary_f[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m     summary_df\u001b[38;5;241m.\u001b[39mto_pickle(summary_f)\n",
      "File \u001b[0;32m~/pycharm/FoodSecurity/DHS/dhs_pca_functions.py:1275\u001b[0m, in \u001b[0;36mcreate_summary_df\u001b[0;34m(file_paths, dataset_type, all_col_labels, cols_to_load_simultaneously, folder_path, create_GEID, create_DHSID, available_since_v)\u001b[0m\n\u001b[1;32m   1271\u001b[0m     partial_summary_df_loading_wrapper \u001b[38;5;241m=\u001b[39m partial(summary_df_loading_wrapper, file_paths\u001b[38;5;241m=\u001b[39mfile_paths, dataset_type\u001b[38;5;241m=\u001b[39mdataset_type, folder_path\u001b[38;5;241m=\u001b[39mfolder_path, all_col_labels\u001b[38;5;241m=\u001b[39mall_col_labels, \n\u001b[1;32m   1272\u001b[0m                                                  available_since_v\u001b[38;5;241m=\u001b[39mavailable_since_v, cols_to_load_simultaneously\u001b[38;5;241m=\u001b[39mcols_to_load_simultaneously, \n\u001b[1;32m   1273\u001b[0m                                                  create_GEID\u001b[38;5;241m=\u001b[39mcreate_GEID, create_DHSID\u001b[38;5;241m=\u001b[39mcreate_DHSID, max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m47\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m-> 1275\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial_summary_df_loading_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_nr_l\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart_nr_l\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m     summary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(results, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;66;03m###Needs to be corrected since not all dfs have the same length\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;66;03m#     summary_df['available_data [%]'] = round((summary_df['available_data amount']/summary_df['available_data amount'].max()) * 100\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;66;03m#     summary_df[\"na_data [%]\"] = 100 - summary_df['available_data [%]']\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;66;03m#     summary_df['available_data V7+ [%]'] = (summary_df['available_data V7+ amount']/summary_df['available_data V7+ amount'].max()) * 100\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;66;03m#     summary_df[\"na_data V7+ [%]\"] = 100 - summary_df['available_data V7+ [%]']\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;66;03m#Some more manipulations\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/datadisk/sharedPrograms/venv/water_2202_tf82/lib/python3.8/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py:484\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    485\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:444\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py:239\u001b[0m, in \u001b[0;36m_process_worker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     r \u001b[38;5;241m=\u001b[39m call_item\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39mcall_item\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_item\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    241\u001b[0m     exc \u001b[38;5;241m=\u001b[39m _ExceptionWithTraceback(e, e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py:198\u001b[0m, in \u001b[0;36m_process_chunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_chunk\u001b[39m(fn, chunk):\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;124;03m\"\"\" Processes a chunk of an iterable passed to map.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    Runs the function passed to map() on a chunk of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [fn(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m chunk]\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_chunk\u001b[39m(fn, chunk):\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;124;03m\"\"\" Processes a chunk of an iterable passed to map.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    Runs the function passed to map() on a chunk of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [fn(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m chunk]\n",
      "File \u001b[0;32m~/pycharm/FoodSecurity/DHS/dhs_pca_functions.py:1254\u001b[0m, in \u001b[0;36msummary_df_loading_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummary_df_loading_wrapper\u001b[39m(start_nr, cols_to_load_simultaneously, file_paths, dataset_type, folder_path, all_col_labels, create_GEID\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, create_DHSID\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1252\u001b[0m                                available_since_v\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m-> 1254\u001b[0m     df \u001b[38;5;241m=\u001b[39m loading_wrapper(start_nr, cols_to_load_simultaneously, file_paths, dataset_type, folder_path, all_col_labels, create_GEID\u001b[38;5;241m=\u001b[39mcreate_GEID, \n\u001b[1;32m   1255\u001b[0m                          create_DHSID\u001b[38;5;241m=\u001b[39mcreate_DHSID, max_workers\u001b[38;5;241m=\u001b[39mmax_workers, create_version_nr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1256\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1257\u001b[0m     summary_df \u001b[38;5;241m=\u001b[39m summarize_columns(df, available_since_v\u001b[38;5;241m=\u001b[39mavailable_since_v)\n",
      "File \u001b[0;32m~/pycharm/FoodSecurity/DHS/dhs_pca_functions.py:1245\u001b[0m, in \u001b[0;36mloading_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1242\u001b[0m         col_labels \u001b[38;5;241m=\u001b[39m col_labels[start_nr:start_nr\u001b[38;5;241m+\u001b[39mcols_to_load_simultaneously]\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;66;03m# ic(col_labels)\u001b[39;00m\n\u001b[0;32m-> 1245\u001b[0m all_data, meta_data \u001b[38;5;241m=\u001b[39m load_data(file_paths, dataset_type, folder_path, col_labels\u001b[38;5;241m=\u001b[39mcol_labels, create_GEID\u001b[38;5;241m=\u001b[39mcreate_GEID, create_DHSID\u001b[38;5;241m=\u001b[39mcreate_DHSID, \n\u001b[1;32m   1246\u001b[0m                                 max_workers\u001b[38;5;241m=\u001b[39mmax_workers, create_version_nr\u001b[38;5;241m=\u001b[39mcreate_version_nr)\n\u001b[1;32m   1247\u001b[0m df \u001b[38;5;241m=\u001b[39m combine_survey_dfs(all_data)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/pycharm/FoodSecurity/DHS/dhs_pca_functions.py:544\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# Using ProcessPoolExecutor\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m--> 544\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(partial_decode_data, files))  \n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# Separating Dask DataFrames and metadata\u001b[39;00m\n\u001b[1;32m    547\u001b[0m dfs, metas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py:484\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m()\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    485\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:619\u001b[0m, in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:444\u001b[0m, in \u001b[0;36mresult\u001b[0;34m()\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36m__get_result\u001b[0;34m()\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py:239\u001b[0m, in \u001b[0;36m_process_worker\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     r \u001b[38;5;241m=\u001b[39m call_item\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39mcall_item\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcall_item\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    241\u001b[0m     exc \u001b[38;5;241m=\u001b[39m _ExceptionWithTraceback(e, e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py:198\u001b[0m, in \u001b[0;36m_process_chunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_chunk\u001b[39m(fn, chunk):\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;124;03m\"\"\" Processes a chunk of an iterable passed to map.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    Runs the function passed to map() on a chunk of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [fn(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m chunk]\n",
      "File \u001b[0;32m/usr/lib/python3.8/concurrent/futures/process.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_chunk\u001b[39m(fn, chunk):\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;124;03m\"\"\" Processes a chunk of an iterable passed to map.\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    Runs the function passed to map() on a chunk of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m \n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [fn(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m chunk]\n",
      "File \u001b[0;32m~/pycharm/FoodSecurity/DHS/dhs_pca_functions.py:429\u001b[0m, in \u001b[0;36mdecode_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase identification\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    428\u001b[0m     ic(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhousehold number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase identification\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m50\u001b[39m))\n\u001b[0;32m--> 429\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon unique cluster + hh nr and non unique case identification - skipping survey\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;66;03m# ic('in', df[['cluster number', 'case identification', 'household number']].sample(20))\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     new_columns \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster number\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcase identification\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(convert_case_id, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist(), \n\u001b[1;32m    433\u001b[0m                 columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster number from case id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhousehold number from case id\u001b[39m\u001b[38;5;124m'\u001b[39m], index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex)\n",
      "\u001b[0;31mValueError\u001b[0m: non unique cluster + hh nr and non unique case identification - skipping survey"
     ]
    }
   ],
   "source": [
    "summary_f = input_dir + 'summary_df_V' + str(min_version) + f'_{dataset_type}_' + '.pkl'\n",
    "#overwrite_pqt = True\n",
    "if not os.path.exists(summary_f) or overwrite_pqt:\n",
    "    summary_df = create_summary_df(file_paths, dataset_type, all_col_labels, cols_to_load_simultaneously, folder_path, create_GEID=False, create_DHSID=False, available_since_v=available_since_v)\n",
    "    summary_df.to_csv(summary_f[:-4] + '.csv', index=False)\n",
    "    summary_df.to_pickle(summary_f)\n",
    "else:\n",
    "    summary_df = pd.read_pickle(summary_f)\n",
    "    #summary_df = summary_df.sort_index()\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79005be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overwrite_pqt = True\n",
    "summary_f_gpt = summary_f[:-4] + '_unify_answers.pkl'\n",
    "if not os.path.exists(summary_f_gpt) or overwrite_pqt:\n",
    "    summary_df = unify_answers_iterator_wrapper(summary_df, input_dir)\n",
    "    # ic(summary_df.shape)\n",
    "    # ic(summary_df['column_name'])\n",
    "    sorted_cols = ['similar_columns nr', 'column_name', 'column_name_normalized', 'numeric values as keys decision', \n",
    "                'available_data [%]', 'numeric [%]', 'string [%]', 'surveys with data in %', 'surveys with data percentage data available','available_data V7+ [%]', 'unique_answers wo keys',\n",
    "                'unified answers 0', 'replace_d unified answers 0', 'unified answers not found 0', 'unified answers not clustered 0',\n",
    "                'unified answers 1', 'replace_d unified answers 1', 'unified answers not found 1', 'unified answers not clustered 1',\n",
    "                'unified answers 2', 'replace_d unified answers 2', 'unified answers not found 2', 'unified answers not clustered 2',\n",
    "                'unified answers 3', 'replace_d unified answers 3', 'unified answers not found 3', 'unified answers not clustered 3',\n",
    "                'unified answers 4', 'replace_d unified answers 4', 'unified answers not found 4', 'unified answers not clustered 4',\n",
    "                'unified answers 5', 'replace_d unified answers 5', 'unified answers not found 5', 'unified answers not clustered 5',\n",
    "                'unified answers 6', 'replace_d unified answers 6', 'unified answers not found 6', 'unified answers not clustered 6',\n",
    "                'unified answers 7', 'replace_d unified answers 7', 'unified answers not found 7', 'unified answers not clustered 7',\n",
    "                'original % overwriting (as % of col2)', \n",
    "                'original % different values (as % of col2) wo keys', \n",
    "                'cosine string similarity answers', 'original different value counts', \n",
    "                'answer mapping for integration', 'answer mapping for integration not found',  'unique_answers',\n",
    "                'integrated % overwriting (as % of col2)', 'integrated % different values (as % of col2) wo keys', \n",
    "                'integrated different value counts', 'replace_d numeric values as keys', 'numeric values as keys not found', 'overall indicator',  \n",
    "                'numeric similarity', 'cosine string similarity indicator', 'mean', 'std', 'string similarity indicator', 'string similarity of non matching answers',\n",
    "                ] \n",
    "    for c in sorted_cols.copy():\n",
    "        if c not in summary_df.columns:\n",
    "            sorted_cols.remove(c)\n",
    "    cols = sorted_cols + [c for c in summary_df.columns if c not in sorted_cols]\n",
    "    summary_df = summary_df[cols]\n",
    "    \n",
    "    # sort by available data\n",
    "    summary_df = summary_df.sort_values(by=['available_data [%]'], ascending=False)\n",
    "    summary_df.to_csv(summary_f_gpt[:-4] + '.csv', index=False)\n",
    "    summary_df.to_pickle(summary_f_gpt)\n",
    "\n",
    "else:\n",
    "    summary_df = pd.read_pickle(summary_f_gpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95036e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similar_columns nr</th>\n",
       "      <th>column_name</th>\n",
       "      <th>column_name_normalized</th>\n",
       "      <th>numeric values as keys decision</th>\n",
       "      <th>available_data [%]</th>\n",
       "      <th>numeric [%]</th>\n",
       "      <th>string [%]</th>\n",
       "      <th>surveys with data in %</th>\n",
       "      <th>surveys with data percentage data available</th>\n",
       "      <th>available_data V7+ [%]</th>\n",
       "      <th>...</th>\n",
       "      <th>data_type</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "      <th>dubious_answers</th>\n",
       "      <th>unique_answers amount</th>\n",
       "      <th>unique_answers wo keys list</th>\n",
       "      <th>unique_answers wo keys dict</th>\n",
       "      <th>replace_d corrected_answers</th>\n",
       "      <th>replace_d corrected_answers w keys</th>\n",
       "      <th>ada_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>number of household members</td>\n",
       "      <td>number</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>98.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.007650680374354124, -0.01233101636171341, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>type of place of residence</td>\n",
       "      <td>type place residence</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>...</td>\n",
       "      <td>categorical</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{2.0: [('2.0: rural', 'rural'), ('2.0: other c...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>[rural, urban, other cities, other urban, yaou...</td>\n",
       "      <td>{'rural': 'rural', 'urban': 'urban', 'other ci...</td>\n",
       "      <td>{'..cotonou': 'cotonou', '..other urban': 'oth...</td>\n",
       "      <td>{'2.0: ..cotonou': 'cotonou', '3.0: ..other ur...</td>\n",
       "      <td>[-0.008523366414010525, 0.0022381108719855547,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61</td>\n",
       "      <td>household number</td>\n",
       "      <td>number</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>77502.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.007650680374354124, -0.01233101636171341, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>cluster number</td>\n",
       "      <td>cluster number</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>61508057.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.011617615818977356, -0.010739745572209358,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>result of household interview</td>\n",
       "      <td>result interview</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.00</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.000</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[completed]</td>\n",
       "      <td>{'completed': 'completed'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.01973319612443447, -0.027247991412878036, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>92</td>\n",
       "      <td>reason for preference of conical-shaped net: o...</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.685</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[no, yes]</td>\n",
       "      <td>{'no': 'no', 'yes': 'yes'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.013715357519686222, -0.014628798700869083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>845</th>\n",
       "      <td>130</td>\n",
       "      <td>reason for preference of conical-shaped net: e...</td>\n",
       "      <td>easier to store when not hung</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.685</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[no, yes]</td>\n",
       "      <td>{'no': 'no', 'yes': 'yes'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.0030202805064618587, 0.001451018382795155, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>91</td>\n",
       "      <td>torch</td>\n",
       "      <td>torch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.685</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[yes, no]</td>\n",
       "      <td>{'yes': 'yes', 'no': 'no'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.00782063789665699, -0.03684137016534805, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>130</td>\n",
       "      <td>reason for preference of rectangular-shaped ne...</td>\n",
       "      <td>easier to hang</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.685</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[no, yes]</td>\n",
       "      <td>{'no': 'no', 'yes': 'yes'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[-0.006558047141879797, 0.01243473868817091, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>134</td>\n",
       "      <td>bed with a mattress</td>\n",
       "      <td>bed with mattress</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.56</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.685</td>\n",
       "      <td>...</td>\n",
       "      <td>numeric</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[no, yes]</td>\n",
       "      <td>{'no': 'no', 'yes': 'yes'}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.014571099542081356, 0.014823011122643948, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>849 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     similar_columns nr                                        column_name  \\\n",
       "0                    61                        number of household members   \n",
       "1                    85                         type of place of residence   \n",
       "2                    61                                   household number   \n",
       "3                     9                                     cluster number   \n",
       "4                   102                      result of household interview   \n",
       "..                  ...                                                ...   \n",
       "844                  92  reason for preference of conical-shaped net: o...   \n",
       "845                 130  reason for preference of conical-shaped net: e...   \n",
       "846                  91                                              torch   \n",
       "847                 130  reason for preference of rectangular-shaped ne...   \n",
       "848                 134                                bed with a mattress   \n",
       "\n",
       "            column_name_normalized numeric values as keys decision  \\\n",
       "0                           number                             NaN   \n",
       "1             type place residence                             NaN   \n",
       "2                           number                             NaN   \n",
       "3                   cluster number                             NaN   \n",
       "4                 result interview                             NaN   \n",
       "..                             ...                             ...   \n",
       "844                          other                             NaN   \n",
       "845  easier to store when not hung                             NaN   \n",
       "846                          torch                             NaN   \n",
       "847                 easier to hang                             NaN   \n",
       "848              bed with mattress                             NaN   \n",
       "\n",
       "     available_data [%]  numeric [%]  string [%]  surveys with data in %  \\\n",
       "0               100.000        100.0         0.0                  100.00   \n",
       "1               100.000          0.0       100.0                  100.00   \n",
       "2               100.000        100.0         0.0                  100.00   \n",
       "3               100.000        100.0         0.0                  100.00   \n",
       "4               100.000          0.0       100.0                  100.00   \n",
       "..                  ...          ...         ...                     ...   \n",
       "844               0.201          0.0         0.2                    0.56   \n",
       "845               0.201          0.0         0.2                    0.56   \n",
       "846               0.201          0.0         0.2                    0.56   \n",
       "847               0.201          0.0         0.2                    0.56   \n",
       "848               0.201          0.0         0.2                    0.56   \n",
       "\n",
       "     surveys with data percentage data available  available_data V7+ [%]  ...  \\\n",
       "0                                          100.0                 100.000  ...   \n",
       "1                                          100.0                 100.000  ...   \n",
       "2                                          100.0                 100.000  ...   \n",
       "3                                          100.0                 100.000  ...   \n",
       "4                                          100.0                 100.000  ...   \n",
       "..                                           ...                     ...  ...   \n",
       "844                                        100.0                   0.685  ...   \n",
       "845                                        100.0                   0.685  ...   \n",
       "846                                        100.0                   0.685  ...   \n",
       "847                                        100.0                   0.685  ...   \n",
       "848                                        100.0                   0.685  ...   \n",
       "\n",
       "       data_type         max  min  \\\n",
       "0        numeric        98.0  1.0   \n",
       "1    categorical         NaN  NaN   \n",
       "2        numeric     77502.0  0.0   \n",
       "3        numeric  61508057.0  1.0   \n",
       "4        numeric         NaN  NaN   \n",
       "..           ...         ...  ...   \n",
       "844      numeric         NaN  NaN   \n",
       "845      numeric         NaN  NaN   \n",
       "846      numeric         NaN  NaN   \n",
       "847      numeric         NaN  NaN   \n",
       "848      numeric         NaN  NaN   \n",
       "\n",
       "                                       dubious_answers  unique_answers amount  \\\n",
       "0                                                  NaN                    NaN   \n",
       "1    {2.0: [('2.0: rural', 'rural'), ('2.0: other c...                   15.0   \n",
       "2                                                  NaN                    NaN   \n",
       "3                                                  NaN                    NaN   \n",
       "4                                                  NaN                    2.0   \n",
       "..                                                 ...                    ...   \n",
       "844                                                NaN                    2.0   \n",
       "845                                                NaN                    2.0   \n",
       "846                                                NaN                    2.0   \n",
       "847                                                NaN                    2.0   \n",
       "848                                                NaN                    2.0   \n",
       "\n",
       "                           unique_answers wo keys list  \\\n",
       "0                                                  NaN   \n",
       "1    [rural, urban, other cities, other urban, yaou...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                          [completed]   \n",
       "..                                                 ...   \n",
       "844                                          [no, yes]   \n",
       "845                                          [no, yes]   \n",
       "846                                          [yes, no]   \n",
       "847                                          [no, yes]   \n",
       "848                                          [no, yes]   \n",
       "\n",
       "                           unique_answers wo keys dict  \\\n",
       "0                                                  NaN   \n",
       "1    {'rural': 'rural', 'urban': 'urban', 'other ci...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                           {'completed': 'completed'}   \n",
       "..                                                 ...   \n",
       "844                         {'no': 'no', 'yes': 'yes'}   \n",
       "845                         {'no': 'no', 'yes': 'yes'}   \n",
       "846                         {'yes': 'yes', 'no': 'no'}   \n",
       "847                         {'no': 'no', 'yes': 'yes'}   \n",
       "848                         {'no': 'no', 'yes': 'yes'}   \n",
       "\n",
       "                           replace_d corrected_answers  \\\n",
       "0                                                  NaN   \n",
       "1    {'..cotonou': 'cotonou', '..other urban': 'oth...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "844                                                NaN   \n",
       "845                                                NaN   \n",
       "846                                                NaN   \n",
       "847                                                NaN   \n",
       "848                                                NaN   \n",
       "\n",
       "                    replace_d corrected_answers w keys  \\\n",
       "0                                                  NaN   \n",
       "1    {'2.0: ..cotonou': 'cotonou', '3.0: ..other ur...   \n",
       "2                                                  NaN   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "844                                                NaN   \n",
       "845                                                NaN   \n",
       "846                                                NaN   \n",
       "847                                                NaN   \n",
       "848                                                NaN   \n",
       "\n",
       "                                         ada_embedding  \n",
       "0    [-0.007650680374354124, -0.01233101636171341, ...  \n",
       "1    [-0.008523366414010525, 0.0022381108719855547,...  \n",
       "2    [-0.007650680374354124, -0.01233101636171341, ...  \n",
       "3    [-0.011617615818977356, -0.010739745572209358,...  \n",
       "4    [-0.01973319612443447, -0.027247991412878036, ...  \n",
       "..                                                 ...  \n",
       "844  [-0.013715357519686222, -0.014628798700869083,...  \n",
       "845  [0.0030202805064618587, 0.001451018382795155, ...  \n",
       "846  [-0.00782063789665699, -0.03684137016534805, -...  \n",
       "847  [-0.006558047141879797, 0.01243473868817091, 0...  \n",
       "848  [0.014571099542081356, 0.014823011122643948, -...  \n",
       "\n",
       "[849 rows x 64 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# overwrite_pqt = True\n",
    "integration_summary_f = input_dir + 'summary_df_V' + str(min_version) + \\\n",
    "    f'_{dataset_type}_clustered' + '.pkl'\n",
    "#summary_f_clustering = input_dir + 'summary_df_V' + str(min_version) + \\\n",
    " #   f'_{dataset_type}_unclustered' + '.pkl'\n",
    "if not os.path.exists(integration_summary_f) or overwrite_pqt:\n",
    "    integration_summary_df = summary_df[summary_df['available_data [%]'] >= 0.2]\n",
    "    ic(integration_summary_df.shape)\n",
    "    integration_summary_df = retrieve_embeddings(integration_summary_df, input_dir)\n",
    "    ic(integration_summary_df.shape)\n",
    "\n",
    "    sorted_cols = ['similar_columns nr', 'column_name', 'column_name_normalized', 'numeric values as keys decision', \n",
    "                   'available_data [%]', 'numeric [%]', 'string [%]', 'surveys with data in %', 'surveys with data percentage data available','available_data V7+ [%]',\n",
    "                'original % overwriting (as % of col2)', \n",
    "                'original % different values (as % of col2) wo keys', \n",
    "                'cosine string similarity answers', 'original different value counts', \n",
    "                'answer mapping for integration', 'answer mapping for integration not found', 'unique_answers wo keys',\n",
    "                'integrated % overwriting (as % of col2)', 'integrated % different values (as % of col2) wo keys', \n",
    "                'integrated different value counts', 'replace_d numeric values as keys', 'numeric values as keys not found', 'overall indicator',  \n",
    "                'numeric similarity', 'cosine string similarity indicator', 'mean', 'std', 'string similarity indicator', 'string similarity of non matching answers',\n",
    "                ] \n",
    "    for c in sorted_cols.copy():\n",
    "        if c not in integration_summary_df.columns:\n",
    "            sorted_cols.remove(c)\n",
    "    ic(sorted_cols)\n",
    "    cols = sorted_cols + [c for c in integration_summary_df.columns if c not in sorted_cols]\n",
    "    integration_summary_df = integration_summary_df[cols]    \n",
    "    integration_summary_df = integration_summary_df[integration_summary_df['similar_columns nr'] != -1]\n",
    "    integration_summary_df = integration_summary_df.reset_index(drop=True)\n",
    "    integration_summary_df_csv = integration_summary_df.drop(columns=['ada_embedding'])\n",
    "    integration_summary_df_csv.to_csv(integration_summary_f[:-4] + '.csv', index=False)\n",
    "    integration_summary_df.to_pickle(integration_summary_f)\n",
    "else:\n",
    "    integration_summary_df = pd.read_pickle(integration_summary_f)\n",
    "integration_summary_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16380e0352645",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:05:16.315972571Z",
     "start_time": "2023-11-15T15:05:07.106869523Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 549.29it/s]\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [02:17<00:00, 22.86s/it] \n",
      "100%|██████████| 3/3 [02:11<00:00, 43.95s/it] \n",
      "100%|██████████| 1/1 [02:10<00:00, 130.45s/it]\n",
      "100%|██████████| 1/1 [02:02<00:00, 122.37s/it]\n",
      "100%|██████████| 1/1 [02:02<00:00, 122.81s/it]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "overwrite_pqt = True\n",
    "integration_summary_f = integration_summary_f[:-4] + '_to_review_manually.pkl'\n",
    "if not os.path.exists(integration_summary_f) or overwrite_pqt:\n",
    "\n",
    "#    %mprun -f process_group_wrapper result_df = process_group_wrapper(integration_summary_df, file_paths, dataset_type, folder_path, input_dir)\n",
    "\n",
    "    integration_summary_df = process_group_wrapper(integration_summary_df, file_paths, dataset_type, folder_path, input_dir)\n",
    "    sorted_cols = ['similar_columns nr', 'column_name', 'column_name_normalized', 'overall indicator short', \n",
    "                   'overall indicator', 'numeric values as keys decision',  'available_data [%]',  'numeric [%]', 'string [%]', \n",
    "                'surveys with data in %', 'surveys with data percentage data available', 'available_data V7+ [%]', 'original % overwriting (as % of col2)', \n",
    "                'original % different values (as % of col2)', \n",
    "                'cosine string similarity answers', 'original different value counts', \n",
    "                'answer mapping for integration', 'answer mapping for integration not found', 'unique_answers wo keys', 'unique_answers',\n",
    "                'integrated % overwriting (as % of col2)', 'integrated % different values (as % of col2)', \n",
    "                'integrated different value counts', 'replace_d numeric values as keys', 'numeric values as keys not found',\n",
    "                'numeric similarity', 'mean', 'std', 'datatype similarity', 'cosine string similarity indicator'] \n",
    "    for c in sorted_cols.copy():\n",
    "        if c not in integration_summary_df.columns:\n",
    "            sorted_cols.remove(c)\n",
    "            ic(c)\n",
    "    ic(sorted_cols)\n",
    "    cols = sorted_cols + [c for c in integration_summary_df.columns if c not in sorted_cols]\n",
    "    integration_summary_df = integration_summary_df[cols]\n",
    "    integration_summary_df_csv = integration_summary_df.drop(columns=['ada_embedding'])\n",
    "    integration_summary_df_csv.to_csv(integration_summary_f[:-4] + '.csv', index=False)\n",
    "    integration_summary_df.to_pickle(integration_summary_f)\n",
    "else:\n",
    "    integration_summary_df = pd.read_pickle(integration_summary_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efbf20cdb10a65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-15T15:05:07.674412229Z",
     "start_time": "2023-11-15T15:05:07.101432468Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18                                                   NaN\n",
       "532                                                  NaN\n",
       "17                 [no, yes, yes public, yes other only]\n",
       "71     [in yard plot, in dwelling, elsewhere, dans la...\n",
       "72                                          [don't know]\n",
       "                             ...                        \n",
       "661                                            [no, yes]\n",
       "790                                         [don't know]\n",
       "825                                            [no, yes]\n",
       "841                [private toilettes, public toilettes]\n",
       "842                           [yes exclusive, no shared]\n",
       "Name: unique_answers wo keys list, Length: 110, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integration_summary_df.loc[:,'unique_answers wo keys list']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "water_2202_tf82",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
