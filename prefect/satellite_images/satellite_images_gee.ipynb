{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap\n",
    "import os\n",
    "import functools\n",
    "from zipfile import ZipFile\n",
    "from dbfread import DBF\n",
    "from dbfread import FieldParser\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from csv import DictReader\n",
    "#from zdrive import Downloader\n",
    "import time\n",
    "\n",
    "#to dos:\n",
    "#please add more comments what a function or a group of functions does\n",
    "#generalize as a script with variables at the top\n",
    "#ensure a whole year of data gets used for 2015 - use 2015-06-01 until 2016-07-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a square around point of interest\n",
    "# Rural : 5.5km Radius\n",
    "# Urban : 2 km Radius\n",
    "def bounding_box(loc, urban_rural, urban_radius, rural_radius):\n",
    "    if urban_rural is 'U'or  urban_rural is 'u':\n",
    "        size = urban_radius\n",
    "    else:\n",
    "        size = rural_radius\n",
    "\n",
    "    intermediate_buffer = loc.buffer(size) #buffer radius, half your box width in m\n",
    "    intermediate_box = intermediate_buffer.bounds() #Draw a bounding box around the circle\n",
    "    return(intermediate_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masking of clouds\n",
    "def maskClouds(img, MAX_CLOUD_PROBABILITY):\n",
    "\n",
    "    clouds = ee.Image(img.get('cloud_mask')).select('probability')\n",
    "    isNotCloud = clouds.lt(MAX_CLOUD_PROBABILITY)\n",
    "    return img.updateMask(isNotCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masking of edges\n",
    "def maskEdges(s2_img):\n",
    "    return s2_img.updateMask(s2_img.select('B8A').mask().updateMask(s2_img.select('B9').mask()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(cluster, survey_name, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY):\n",
    "    \n",
    "    #Get images collections\n",
    "    s2Sr = ee.ImageCollection('COPERNICUS/S2')\n",
    "    s2Clouds = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "\n",
    "    #Get time span\n",
    "    year_uncut = str(cluster[\"year\"])\n",
    "    year = year_uncut[:year_uncut.rfind('.')]\n",
    "    if int(year)<2016:\n",
    "        START_DATE = ee.Date('2015-06-01')\n",
    "        END_DATE = ee.Date('2016-07-01')\n",
    "    else:\n",
    "        START_DATE = ee.Date(year+'-01-01')\n",
    "        END_DATE = ee.Date(year+'-12-31')\n",
    "    \n",
    "   \n",
    "    #Point of interest (longitude, latidude)\n",
    "    lat_float = float(cluster[\"latidude\"])\n",
    "    lon_float = float(cluster[\"longitude\"])                 \n",
    "    loc = ee.Geometry.Point([lon_float, lat_float])\n",
    "    #Region of interest\n",
    "    region = bounding_box(loc, cluster['urban_rural'], urban_radius, rural_radius)\n",
    "\n",
    "    # Filter input collections by desired data range and region.\n",
    "    #criteria = ee.Filter.And(ee.Filter.bounds(region), ee.Filter.date(START_DATE, END_DATE))\n",
    "    #s2Sr = s2Sr.filter(criteria).map(maskEdges)\n",
    "    #s2Clouds = s2Clouds.filter(criteria)\n",
    "    s2Sr = s2Sr.filterBounds(region).filterDate(START_DATE, END_DATE).map(maskEdges)\n",
    "    s2Clouds = s2Clouds.filterBounds(region).filterDate(START_DATE, END_DATE)\n",
    "\n",
    "    # Join S2 with cloud probability dataset to add cloud mask.\n",
    "    s2SrWithCloudMask = ee.Join.saveFirst('cloud_mask').apply(\n",
    "      primary =  s2Sr, \n",
    "      secondary = s2Clouds, \n",
    "      condition = ee.Filter.equals(\n",
    "          leftField =  'system:index', rightField = 'system:index') \n",
    "        )\n",
    "\n",
    "    maskCloudsWithProb = functools.partial(maskClouds, MAX_CLOUD_PROBABILITY = MAX_CLOUD_PROBABILITY)\n",
    "    s2CloudMasked = ee.ImageCollection(s2SrWithCloudMask).map(maskCloudsWithProb).median()\n",
    "    s2CloudMasked = s2CloudMasked.select(['B1','B2','B3', 'B4', 'B5', 'B6', 'B7', 'B8','B8A', 'B9', 'B10'\\\n",
    "                                         ,'B11','B12']).clip(region)\n",
    "    #Saving location/directory\n",
    "    #out_dir = os.path.join(survey_dir, cluster[\"ID-cluster\"]+'.tif')\n",
    "    #geemap.ee_export_image(s2CloudMasked, filename=out_dir, scale=10)\n",
    "    filename = cluster[\"ID-cluster\"]\n",
    "    filename = filename.replace(filename[:6], survey_name)\n",
    "    task = ee.batch.Export.image.toDrive(s2CloudMasked, description = filename, folder = 'sentinel', scale = 10)\n",
    "    task.start()\n",
    "    print('Created', filename)\n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract solely the dbf-file from the zip and save them into a seperate folder\n",
    "def get_dbf(listOfFileNames, newpath, zipObject):\n",
    "\n",
    "    for fileName in listOfFileNames:\n",
    "        if fileName.endswith('.dbf') or fileName.endswith('.DBF'):\n",
    "            # Extract a single file from zip\n",
    "            zipObject.extract(fileName, newpath)\n",
    "            \n",
    "#If within a zip folder other zip folders included take/work with the the zip folder which is bigger            \n",
    "def extract_bigger_zip(zip_dir,filenames,newpath):\n",
    "    zips_dir = zip_dir[:zip_dir.find('.')]\n",
    "    if not os.path.exists(zips_dir):\n",
    "        os.makedirs(zips_dir)\n",
    "    with ZipFile(zip_dir, 'r') as zipObj:\n",
    "    #Extract all the contents of zip file in different directory\n",
    "        zipObj.extractall(zips_dir)\n",
    "\n",
    "    big_size = 0\n",
    "    big_zip = None\n",
    "    for item in filenames:\n",
    "        if item.endswith('.zip') or item.endswith('.ZIP'):\n",
    "            file_dir = os.path.join(zips_dir, item)\n",
    "            curr_size= os.stat(file_dir).st_size\n",
    "            if curr_size >= big_size:\n",
    "                big_size = curr_size\n",
    "                big_zip = file_dir\n",
    "    check_dbf(big_zip,newpath)    \n",
    "    \n",
    "#Second mainpart for single zip file        \n",
    "def check_dbf(zip_dir, newpath):\n",
    "    big_zip = None\n",
    "    with ZipFile(zip_dir, 'r') as zipObject:\n",
    "        listOfFileNames = zipObject.namelist()\n",
    "\n",
    "        if any((element.endswith('.dbf') or element.endswith('.DBF')) for element in listOfFileNames):\n",
    "            get_dbf(listOfFileNames, newpath, zipObject)\n",
    "        elif any((element.endswith('.zip') or element.endswith('.ZIP')) for element in listOfFileNames):\n",
    "            extract_bigger_zip(zip_dir, listOfFileNames, newpath)  \n",
    "\n",
    "#Delete all directories which may be created during the process to figure out which zip folder is bigger             \n",
    "def delete_zip_folders(dir_name,newpath):\n",
    "    list_subfolders_with_paths = [f.path for f in os.scandir(dir_name) if f.is_dir()]\n",
    "    for folder in list_subfolders_with_paths:\n",
    "        if not folder == newpath:\n",
    "            shutil.rmtree(folder)\n",
    "            \n",
    "#Main part-> runs through all zip files in directory  \n",
    "def extract_dbf(dir_name, newpath):\n",
    "    #Create folder for DBF files\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    folder = os.listdir(dir_name)\n",
    "    #Extract dbf file (if existing) from each zip file\n",
    "    for item in folder:\n",
    "        if item.endswith('.zip') or item.endswith('.ZIP'):\n",
    "            zip_dir = os.path.join(dir_name, item)\n",
    "            check_dbf(zip_dir, newpath)\n",
    "            \n",
    "    delete_zip_folders(dir_name,newpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed as a file may contain \\x00\\ which the standard libary is not able to process, hence an additional class based on the following issue https://github.com/olemb/dbfread/issues/20 is used \n",
    "\n",
    "class MyFieldParser(FieldParser):\n",
    "    def parseN(self, field, data):\n",
    "        data = data.strip().strip(b'*\\x00')  # Had to strip out the other characters first before \\x00, as per super function specs.\n",
    "        return super(MyFieldParser, self).parseN(field, data)\n",
    "\n",
    "    def parseD(self, field, data):\n",
    "        data = data.strip(b'\\x00')\n",
    "        return super(MyFieldParser, self).parseD(field, data)\n",
    "    \n",
    "    \n",
    "#Create single CSV file    \n",
    "def get_csv(dbf_path, dir_csv):\n",
    "\n",
    "    clusters = list()\n",
    "    filename = os.path.basename(dbf_path[:dbf_path.rfind('.')])\n",
    "    if not filename.endswith('SR'):\n",
    "        table = DBF(dbf_path, parserclass=MyFieldParser)\n",
    "        for record in table:\n",
    "        \n",
    "            cluster = {\"ID-survey\": filename, \"ID-cluster\" : record['DHSID'], \"cluster\": record['DHSCLUST'],\\\n",
    "                        \"year\": record[\"DHSYEAR\"],\"urban_rural\": record['URBAN_RURA'], \"latidude\": record[\"LATNUM\"],\\\n",
    "                      \"longitude\": record['LONGNUM']}\n",
    "            clusters.append(cluster)\n",
    "    \n",
    "        clust_df = pd.DataFrame(clusters)\n",
    "    \n",
    "        export_name = filename+'.csv'\n",
    "        export_dir = os.path.join(dir_csv, export_name)\n",
    "\n",
    "        clust_df.to_csv(export_dir)\n",
    "    else:\n",
    "        print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CSV files for each survey (with dbf file); Extract information from dbf file   \n",
    "def create_csv(dir_dbf, dir_csv):\n",
    "    if not os.path.exists(dir_csv):\n",
    "        os.makedirs(dir_csv)\n",
    "\n",
    "    directory = os.listdir(dir_dbf)\n",
    "    for file in directory: \n",
    "        #print(\"This is the file\", file)\n",
    "        dbf_path = os.path.join (dir_dbf, file)\n",
    "        get_csv(dbf_path, dir_csv)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move all csv-files from surveys which took place before 2013 to a seperate folder\n",
    "def before_2013 (export_path, before_2013):\n",
    "    \n",
    "    if not os.path.exists(before_2013):\n",
    "        os.makedirs(before_2013)\n",
    "\n",
    "    directory = os.listdir(export_path)    \n",
    "    for file in directory:\n",
    "        #print(file)\n",
    "        if file.endswith('.csv'):\n",
    "            csv_file = os.path.join(export_path, file)\n",
    "            survey_year = pd.read_csv(csv_file, usecols = ['year'])\n",
    "\n",
    "            if survey_year['year'].max()< 2013:\n",
    "                new_path = os.path.join(before_2013, file)\n",
    "                os.rename(csv_file, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_survey_images(file_dir, survey_name, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY):\n",
    "    with open(file_dir, 'r') as read_obj:\n",
    "    # pass the file object to DictReader() to get the DictReader object\n",
    "        dict_reader = DictReader(read_obj)\n",
    "    # get a list of dictionaries from dct_reader\n",
    "        clusters = list(dict_reader)\n",
    "        \n",
    "    for cluster in clusters:\n",
    "        loc = get_image(cluster, survey_name, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY)\n",
    "        time.sleep(45)\n",
    "'''   \n",
    "def download_local(survey_dir):\n",
    "    output_directory = survey_dir\n",
    "    d = Downloader()\n",
    "\n",
    "    # folder which want to download from Drive\n",
    "    folder_id = '1ST67vgoNlfuClI-zPlEp4F38JsnaM441'\n",
    "    d.downloadFolder(folder_id, destinationFolder=output_directory)\n",
    "'''    \n",
    "#Main functions for getting the sentinel images; here: only the directory for each survey is created     \n",
    "def sentinel_img_survey(img_dir, csv_dir, sentinel_done, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY):\n",
    "    if not os.path.exists(img_dir):\n",
    "        os.makedirs(img_dir)\n",
    "        \n",
    "    csv_directory = os.listdir(csv_dir)\n",
    "    img_directory = os.listdir(img_dir)\n",
    "    \n",
    "    for file in csv_directory:\n",
    "        if file.endswith('.csv'):\n",
    "            filename  = file[:file.rfind('.')]\n",
    "            #Check if survey is already done we skip this survey (sentinel_done file has to be edited manually)\n",
    "            with open(sentinel_done) as f:\n",
    "                if not filename in f.read():\n",
    "                    survey_name = file[:file.rfind('.')]\n",
    "                    survey_dir = os.path.join(img_dir, survey_name)\n",
    "                    if not os.path.exists(survey_dir):\n",
    "                        os.makedirs(survey_dir)\n",
    "                    file_dir = os.path.join(csv_dir, file)\n",
    "                    get_survey_images(file_dir, survey_name, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY)\n",
    "                    #download_local(survey_dir)\n",
    "                    print(file, 'finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted dbf files\n",
      "CDGE71FLSR\n",
      "KEGE6AFLSR\n",
      "MWGE6IFLSR\n",
      "NMGE6AFLSR\n",
      "SNGE6IFLSR\n",
      "SNGE71FLSR\n",
      "SNGE7AFLSR\n",
      "SNGE7IFLSR\n",
      "SNGE7RFLSR\n",
      "TZGE71FLSR\n",
      "created csv data\n",
      "Moved all surveys from before 2013 into seperate folder\n",
      "Created AOGE71FL00000001\n",
      "Created AOGE71FL00000002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-10277048bd80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mbefore_2013\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbefore_2013_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Moved all surveys from before 2013 into seperate folder'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0msentinel_img_survey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentinel_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murban_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrural_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_CLOUD_PROBABILITY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-27a5e19f3ab5>\u001b[0m in \u001b[0;36msentinel_img_survey\u001b[0;34m(img_dir, csv_dir, sentinel_done, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY)\u001b[0m\n\u001b[1;32m     37\u001b[0m                         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurvey_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                     \u001b[0mfile_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     \u001b[0mget_survey_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurvey_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murban_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrural_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_CLOUD_PROBABILITY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                     \u001b[0;31m#download_local(survey_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'finished'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-27a5e19f3ab5>\u001b[0m in \u001b[0;36mget_survey_images\u001b[0;34m(file_dir, survey_name, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurvey_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murban_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrural_radius\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_CLOUD_PROBABILITY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m '''   \n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownload_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurvey_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Main Part\n",
    "\n",
    "#Parameter\n",
    "urban_radius = 1000\n",
    "rural_radius = 5000\n",
    "MAX_CLOUD_PROBABILITY = 20\n",
    "\n",
    "\n",
    "#Paths\n",
    "zip_dir = \"/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/SatelliteImage__GEE/correlation/GPS_Data\"\n",
    "dbf_dir = os.path.join(zip_dir, \"dbf_files\")\n",
    "csv_dir = os.path.join(zip_dir, \"gps_csv\")\n",
    "before_2013_dir = os.path.join(csv_dir, \"before_2013\")\n",
    "#if directly to local computer\n",
    "#img_dir = os.path.join(csv_dir, \"tif_data\")\n",
    "img_dir = '/run/media/shannon/TOSHIBA/Sentinel'\n",
    "sentinel_done = os.path.join(zip_dir, \"sentinel_done.txt\")\n",
    "\n",
    "\n",
    "#Functions\n",
    "extract_dbf(zip_dir, dbf_dir)\n",
    "print('Extracted dbf files')\n",
    "create_csv(dbf_dir, csv_dir)\n",
    "print('created csv data')\n",
    "before_2013(csv_dir, before_2013_dir)\n",
    "print('Moved all surveys from before 2013 into seperate folder')\n",
    "sentinel_img_survey(img_dir, csv_dir, sentinel_done, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
