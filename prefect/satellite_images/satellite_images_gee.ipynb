{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11093cc",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "This code downloads Sentinel 2 Images belonging to DHS Survey clusters to your Google Drive.\n",
    "To download the images for a specific DHS-Survey you need the Geodata zipfile belonging to it. This zipfile needs to have a dbf-file which is used to extract the location and the residence type (urban or rural).\n",
    "Please note that cluster belonging to the refugee type will be ignored (only very few cluster belong to this type). Furthermore, to avoid that the Google Drive runs out of memory space, you may use the gdrive programs (also uploaded to github).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3509162e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10 (default, Jun  2 2021, 10:49:15) \n",
      "[GCC 9.4.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "055bc6cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ee\n",
    "#Authenticate may be require the first time, afterwards usually (hence at the beginning uncommenting may be desired)\n",
    "#ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d7d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "import os\n",
    "import functools\n",
    "from zipfile import ZipFile\n",
    "from dbfread import DBF\n",
    "from dbfread import FieldParser\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from csv import DictReader\n",
    "import time\n",
    "\n",
    "#to dos:\n",
    "#please add more comments what a function or a group of functions does\n",
    "#generalize as a script with variables at the top\n",
    "#ensure a whole year of data gets used for 2015 - use 2015-06-01 until 2016-07-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474857e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a square around point of interest\n",
    "def bounding_box(loc:ee.geometry.Geometry, urban_rural:str, urban_radius:int, rural_radius:int):\n",
    "    #Different size of the box for getting the sentinel images depending on the residence type due to different scattering (see DHS Survey)\n",
    "    if urban_rural == 'U'or  urban_rural == 'u':\n",
    "        size = urban_radius\n",
    "    elif urban_rural == 'R' or urban_rural =='r':\n",
    "        size = rural_radius\n",
    "\n",
    "    intermediate_buffer = loc.buffer(size) #buffer radius, half your box width in m\n",
    "    intermediate_box = intermediate_buffer.bounds() #Draw a bounding box around the circle\n",
    "    \n",
    "    return(intermediate_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e3d3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masking of clouds -> Filters pixel w.r.t to their probability of being cloudy or not \n",
    "def maskClouds(img:ee.image.Image, MAX_CLOUD_PROBABILITY:int):\n",
    "    \n",
    "    clouds = ee.Image(img.get('cloud_mask')).select('probability')\n",
    "    isNotCloud = clouds.lt(MAX_CLOUD_PROBABILITY)\n",
    "    \n",
    "    return img.updateMask(isNotCloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114f20e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masking of edges\n",
    "def maskEdges(s2_img:ee.image.Image):\n",
    "    return s2_img.updateMask(s2_img.select('B8A').mask().updateMask(s2_img.select('B9').mask()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e038ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(cluster:dict, survey_name:str, urban_radius:int, rural_radius:int, MAX_CLOUD_PROBABILITY:int):\n",
    "    '''Main function to extract a single Sentinel2  image given a cluster dictionary containing the \n",
    "    latidude, longitude; the output is an image constructed from multiple Sentinel images such that the output image\n",
    "    is as cloudless as possible given a certain timeframe'''\n",
    "    \n",
    "    #Get images collections\n",
    "    s2Sr = ee.ImageCollection('COPERNICUS/S2')\n",
    "    s2Clouds = ee.ImageCollection('COPERNICUS/S2_CLOUD_PROBABILITY')\n",
    "\n",
    "    #Get time span -> if survey year is before 2016 we use a predefined time frame as given below, because the acquistion\n",
    "    # of sentinel data started in May 2015\n",
    "    year_uncut = str(cluster[\"year\"])\n",
    "    year = year_uncut[:year_uncut.rfind('.')]\n",
    "    if int(year)<2016:\n",
    "        START_DATE = ee.Date('2015-06-01')\n",
    "        END_DATE = ee.Date('2016-07-01')\n",
    "    else:\n",
    "        START_DATE = ee.Date(year+'-01-01')\n",
    "        END_DATE = ee.Date(year+'-12-31')\n",
    "    \n",
    "   \n",
    "    #Point of interest (longitude, latidude); this is the center of the output image \n",
    "    lat_float = float(cluster[\"latidude\"])\n",
    "    lon_float = float(cluster[\"longitude\"])                 \n",
    "    loc = ee.Geometry.Point([lon_float, lat_float])\n",
    "    #Region of interest; around the point of interest we define the region (size depends on rural or urban type)\n",
    "    # the region of interest is a square\n",
    "    region = bounding_box(loc, cluster['urban_rural'], urban_radius, rural_radius)\n",
    "\n",
    "    # Filter input collections by desired data range and region.\n",
    "    s2Sr = s2Sr.filterBounds(region).filterDate(START_DATE, END_DATE).map(maskEdges)\n",
    "    s2Clouds = s2Clouds.filterBounds(region).filterDate(START_DATE, END_DATE)\n",
    "\n",
    "    # Join S2 with cloud probability dataset to add cloud mask.\n",
    "    s2SrWithCloudMask = ee.Join.saveFirst('cloud_mask').apply(\n",
    "      primary =  s2Sr, \n",
    "      secondary = s2Clouds, \n",
    "      condition = ee.Filter.equals(\n",
    "          leftField =  'system:index', rightField = 'system:index') \n",
    "        )\n",
    "    \n",
    "    #Masking the image\n",
    "    maskCloudsWithProb = functools.partial(maskClouds, MAX_CLOUD_PROBABILITY = MAX_CLOUD_PROBABILITY)\n",
    "    s2CloudMasked = ee.ImageCollection(s2SrWithCloudMask).map(maskCloudsWithProb).median()\n",
    "    #Select which bands to keep\n",
    "    s2CloudMasked = s2CloudMasked.select(['B1','B2','B3', 'B4', 'B5', 'B6', 'B7', 'B8','B8A', 'B9', 'B10'\\\n",
    "                                         ,'B11','B12']).clip(region)\n",
    "    \n",
    "    #Saving location/directory to Google Drive (#commented part if you want to save it locally directly, but the\n",
    "    #size of the images has it maximum size much lower compared to downloading it to Google Drive first)\n",
    "    #-> name of the img is the surveyname+0's+cluster_number\n",
    "    #out_dir = os.path.join(survey_dir, cluster[\"ID-cluster\"]+'.tif')\n",
    "    #geemap.ee_export_image(s2CloudMasked, filename=out_dir, scale=10)\n",
    "    filename = cluster[\"ID-cluster\"]\n",
    "    filename = filename.replace(filename[:6], survey_name)\n",
    "    task = ee.batch.Export.image.toDrive(s2CloudMasked, description = filename, folder = 'sentinel', scale = 10)\n",
    "    task.start()\n",
    "    print('Created', filename)\n",
    "    \n",
    "    \n",
    "    return loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9199c376",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'zipfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2939/3775222319.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_dbf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlistOfFileNames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzipObject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Extract solely the dbf-file from the zip and save them into a seperate folder defined before (newpath)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistOfFileNames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.dbf'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfileName\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.DBF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zipfile' is not defined"
     ]
    }
   ],
   "source": [
    "def get_dbf(listOfFileNames:list, newpath:str, zipObject:zipfile.ZipFile):\n",
    "    \n",
    "    # Extract solely the dbf-file from the zip and save them into a seperate folder defined before (newpath)\n",
    "    for fileName in listOfFileNames:\n",
    "        if fileName.endswith('.dbf') or fileName.endswith('.DBF'):\n",
    "            # Extract a single file from zip\n",
    "            zipObject.extract(fileName, newpath)\n",
    "            \n",
    "#For the case if a zip folder includes zip folders: Take zip folder to get dbf within main zip folder which is bigger in size (bytes)           \n",
    "def extract_bigger_zip(zip_dir:str,filenames:str,newpath:str):\n",
    "    \n",
    "    #Create temporary directory for this survey\n",
    "    zips_dir = zip_dir[:zip_dir.find('.')]\n",
    "    if not os.path.exists(zips_dir):\n",
    "        os.makedirs(zips_dir)\n",
    "        \n",
    "    #Extract the zip folders into the directory    \n",
    "    with ZipFile(zip_dir, 'r') as zipObj:\n",
    "    #Extract all the contents of zip file in different directory\n",
    "        zipObj.extractall(zips_dir)\n",
    "\n",
    "    big_size = 0\n",
    "    big_zip = None\n",
    "    \n",
    "    #Compare zip folders and take the biggest one\n",
    "    for item in filenames:\n",
    "        if item.endswith('.zip') or item.endswith('.ZIP'):\n",
    "            file_dir = os.path.join(zips_dir, item)\n",
    "            curr_size= os.stat(file_dir).st_size\n",
    "            if curr_size >= big_size:\n",
    "                big_size = curr_size\n",
    "                big_zip = file_dir\n",
    "    \n",
    "    #Now check via check_dbf if dbf-file is given or if you have again zip-files within the current zip file\n",
    "    check_dbf(big_zip,newpath)    \n",
    "    \n",
    "#Second mainpart for single zip file        \n",
    "def check_dbf(zip_dir:str, newpath:str):\n",
    "    \n",
    "    big_zip = None\n",
    "\n",
    "    #Get list of all elements within the zipfile\n",
    "    with ZipFile(zip_dir, 'r') as zipObject:\n",
    "        listOfFileNames = zipObject.namelist()\n",
    "\n",
    "        #Check if any element within the zip is a dbf file if yes jump into function get_dbf\n",
    "        if any((element.endswith('.dbf') or element.endswith('.DBF')) for element in listOfFileNames):\n",
    "            get_dbf(listOfFileNames, newpath, zipObject)\n",
    "        \n",
    "        #If \"if-clause\" not true, check here if any element is a zip file if yes jump into extract_bigger_zip\n",
    "        elif any((element.endswith('.zip') or element.endswith('.ZIP')) for element in listOfFileNames):\n",
    "            extract_bigger_zip(zip_dir, listOfFileNames, newpath)  \n",
    "\n",
    "def delete_zip_folders(dir_name,newpath):\n",
    "    #Delete all directories which may be created during the process to figure out which zip folder is bigger             \n",
    "    list_subfolders_with_paths = [f.path for f in os.scandir(dir_name) if f.is_dir()]\n",
    "    for folder in list_subfolders_with_paths:\n",
    "        if not folder == newpath:\n",
    "            shutil.rmtree(folder)\n",
    "            \n",
    "#Main part-> runs through all zip files in the directory  \n",
    "def extract_dbf(dir_name:str, newpath:str):\n",
    "    '''The GEO DHS Survey retrieved contain usually a dbf-file from which we can gather all relevant information\n",
    "    such as location cluster name and residence type; this part retrieves the dbf files, extracts the relevant\n",
    "    information and saves them as csv file for each survey'''\n",
    "    \n",
    "    #Create folder for DBF files\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    #List of all survey zips in the directory dir_name    \n",
    "    folders = os.listdir(dir_name)\n",
    "    \n",
    "    #Extract dbf file (if existing) from each zip file via check_dbf\n",
    "    for item in folders:\n",
    "        if item.endswith('.zip') or item.endswith('.ZIP'):\n",
    "            zip_dir = os.path.join(dir_name, item)\n",
    "            check_dbf(zip_dir, newpath)\n",
    "     \n",
    "    #Delete all directories via the following function\n",
    "    delete_zip_folders(dir_name,newpath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b45b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed as a file may contain \\x00\\ which the standard libary is not able to process, hence an additional class based on the following issue https://github.com/olemb/dbfread/issues/20 is used \n",
    "class MyFieldParser(FieldParser):\n",
    "    def parseN(self, field, data):\n",
    "        data = data.strip().strip(b'*\\x00')  # Had to strip out the other characters first before \\x00, as per super function specs.\n",
    "        return super(MyFieldParser, self).parseN(field, data)\n",
    "\n",
    "    def parseD(self, field, data):\n",
    "        data = data.strip(b'\\x00')\n",
    "        return super(MyFieldParser, self).parseD(field, data)\n",
    "    \n",
    "    \n",
    "#Create single CSV file for the current dbf file belonging to a specific survey (dbf_path)   \n",
    "def get_csv(dbf_path:str, dir_csv:str):\n",
    "\n",
    "    clusters = list()\n",
    "    \n",
    "    #get surveyname\n",
    "    filename = os.path.basename(dbf_path[:dbf_path.rfind('.')])\n",
    "    \n",
    "    #Ignore all filenames ending with SR as they are not containing the necessary information required to retrived the\n",
    "    #sentinel data later one\n",
    "    if not filename.endswith('SR'):\n",
    "        #read in dbf file\n",
    "        table = DBF(dbf_path, parserclass=MyFieldParser)\n",
    "        #Extract for each record/cluster of the survey the relevant information and add them to the cluster list\n",
    "        for record in table:\n",
    "            cluster = {\"ID-survey\": filename, \"ID-cluster\" : record['DHSID'], \"cluster\": record['DHSCLUST'],\\\n",
    "                        \"year\": record[\"DHSYEAR\"],\"urban_rural\": record['URBAN_RURA'], \"latidude\": record[\"LATNUM\"],\\\n",
    "                      \"longitude\": record['LONGNUM']}\n",
    "            clusters.append(cluster)\n",
    "        \n",
    "        #Transform cluster list into pandas dataframe\n",
    "        clust_df = pd.DataFrame(clusters)\n",
    "        \n",
    "        #Define export directory; the name of the csv file is the survey name (identically wit the zip file name)\n",
    "        export_name = filename+'.csv'\n",
    "        export_dir = os.path.join(dir_csv, export_name)\n",
    "        #Save the cluster dataframe to the defined export directory\n",
    "        clust_df.to_csv(export_dir)\n",
    "    else:\n",
    "        #Print all surveys/dbf files where no csv file was creates as the dbf files were ending on SR\n",
    "        print(filename)\n",
    "\n",
    "\n",
    "def create_csv(dir_dbf:str, dir_csv:str):\n",
    "    \n",
    "    '''Create CSV files for each survey (if they have a dbf file); The information for the csv files are extracted\n",
    "    from the dbf file; the final csv file contains the following columns ID-survey (survey name), ID-cluster (cluster name)\n",
    "    , cluster (cluster number), urban_rural (the residence type), latitude (latidude geolocation), and longitude\n",
    "    (the longitudal geolocation); for each cluster of the survey there is a seperate row'''\n",
    "    \n",
    "    #Create the general Csv-directory where all csv files are stored\n",
    "    if not os.path.exists(dir_csv):\n",
    "        os.makedirs(dir_csv)\n",
    "    \n",
    "    #List of all paths to the dbf files\n",
    "    directory = os.listdir(dir_dbf)\n",
    "    \n",
    "    #Iterating through all dbf files and call the function get_csv\n",
    "    for file in directory: \n",
    "        #print(\"This is the file\", file)\n",
    "        dbf_path = os.path.join (dir_dbf, file)\n",
    "        get_csv(dbf_path, dir_csv)\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae7c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move all csv-files from surveys which took place before 2013 to a seperate folder\n",
    "def before_2013 (export_path:str, before_2013:str):\n",
    "    \n",
    "    if not os.path.exists(before_2013):\n",
    "        os.makedirs(before_2013)\n",
    "\n",
    "    directory = os.listdir(export_path)    \n",
    "    for file in directory:\n",
    "        \n",
    "        if file.endswith('.csv'):\n",
    "            csv_file = os.path.join(export_path, file)\n",
    "            survey_year = pd.read_csv(csv_file, usecols = ['year'])\n",
    "            #Move csv-file to seperate folder if the year <2013\n",
    "            if survey_year['year'].max()< 2013:\n",
    "                new_path = os.path.join(before_2013, file)\n",
    "                os.rename(csv_file, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3289a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_survey_images(file_dir:str, survey_name:str, urban_radius:int, rural_radius:int, MAX_CLOUD_PROBABILITY:int):\n",
    "    \n",
    "    with open(file_dir, 'r') as read_obj:\n",
    "    # pass the file object to DictReader() to get the DictReader object\n",
    "        dict_reader = DictReader(read_obj)\n",
    "    # get a list of dictionaries from dct_reader\n",
    "        clusters = list(dict_reader)\n",
    "    #Iterate through all clusters of the survey and execute get_image for each cluster to retrieve its corresponding sentinel image    \n",
    "    for cluster in clusters:\n",
    "        if not (cluster[\"latidude\"] == 0 and cluster[\"longitude\"]):\n",
    "            loc = get_image(cluster, survey_name, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY)\n",
    "            #Wait a little bit (currently 45 s) until the code is further executed -> Reason: GEE takes a while to\n",
    "            #retrieve the images. Consequently, to avoid too many queries to GEE at the same time and hence\n",
    "            #, the undesirable stopping of the program, we need to implement this waiting function\n",
    "            time.sleep(45)\n",
    "\n",
    "#Main functions for getting the sentinel images; here: only the directory for each survey is created     \n",
    "def sentinel_img_survey(img_dir:str, csv_dir:str, sentinel_done:str, urban_radius:int, rural_radius:int, MAX_CLOUD_PROBABILITY:int):\n",
    "    '''Here is the main part to retrieve sentinel images (including the subfunctions); please not that with img_dir\n",
    "    the diretory is created which should contain the zip folders for each survey containing the sentinel images for each\n",
    "    cluster; however within this program only the directory and the temporary folders for each survey are created;\n",
    "    the images are saved temporarily in a different directory via the gdrive-scripts as they need to be retrieved from\n",
    "    Google Drive; furthermore after downloading all imags locally into this seperate directory you need to run \n",
    "    the program Sorting_sentinel-img_into_zips.ipynb to group the images w.r.t to their survey and create the zip folders'''\n",
    "    if not os.path.exists(img_dir):\n",
    "        os.makedirs(img_dir)\n",
    "        \n",
    "    #Create (if not already existing) sentinel_done txt file. This file stores the names of all surveys where\n",
    "    #the images were already retrieved to skip them if the program is started again\n",
    "    if not os.path.isfile(sentinel_done):\n",
    "        open(sentinel_done, 'a').close()\n",
    "     \n",
    "    csv_directory = os.listdir(csv_dir)\n",
    "    img_directory = os.listdir(img_dir)\n",
    "    \n",
    "    for file in csv_directory:\n",
    "        if file.endswith('.csv'):\n",
    "            filename  = file[:file.rfind('.')]\n",
    "            #If survey is already done we skip this survey\n",
    "            with open(sentinel_done) as f:\n",
    "                if not filename in f.read():\n",
    "                    #Create survey folder within the final directory of the images (zip folder directory see above)\n",
    "                    survey_name = file[:file.rfind('.')]\n",
    "                    survey_dir = os.path.join(img_dir, survey_name)\n",
    "                    if not os.path.exists(survey_dir):\n",
    "                        os.makedirs(survey_dir)\n",
    "                    \n",
    "                    #Retrieve the images via get_survey_images for current survey\n",
    "                    file_dir = os.path.join(csv_dir, file)\n",
    "                    get_survey_images(file_dir, survey_name, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY)\n",
    "                    \n",
    "                    #Add survey to txt file which stores all surveys which are done to avoid downloading them again if you reload the program\n",
    "                    file1 = open(sentinel_done,\"w\")#write mode\n",
    "                    file1.write(file+\"\\n\")\n",
    "                    file1.close()\n",
    "                    \n",
    "                    print(file, 'finished')\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05f94e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Part\n",
    "\n",
    "#Parameter\n",
    "urban_radius = 1000 # meter\n",
    "rural_radius = 5000 # meter\n",
    "MAX_CLOUD_PROBABILITY = 20 # %\n",
    "\n",
    "\n",
    "#Paths\n",
    "#Directory of originally zip_files containing the dbf-files\n",
    "zip_dir = \"/mnt/datadisk/shannon/get_sentinel/GPS_Data\"\n",
    "#Directory where dbf files are stored\n",
    "dbf_dir = os.path.join(zip_dir, \"dbf_files\")\n",
    "#Directory where csv files are stored\n",
    "csv_dir = os.path.join(zip_dir, \"gps_csv\")\n",
    "#Subdirectory of csv_dir where all csv files are stored which belongs to surveys carried out before 2013\n",
    "before_2013_dir = os.path.join(csv_dir, \"before_2013\")\n",
    "#if directly to local computer\n",
    "#img_dir = os.path.join(csv_dir, \"tif_data\")\n",
    "#Directory where the final survey zips containing the sentinel images are stores\n",
    "img_dir = '/mnt/datadisk/shannon/get_sentinel/sentinel_images_zip'\n",
    "#Directory to txt files which contains all surveys where the images were already retrieved\n",
    "sentinel_done = os.path.join(zip_dir, \"sentinel_done.txt\")\n",
    "\n",
    "\n",
    "#Functions\n",
    "extract_dbf(zip_dir, dbf_dir)\n",
    "print('Extracted dbf files')\n",
    "create_csv(dbf_dir, csv_dir)\n",
    "print('created csv data')\n",
    "before_2013(csv_dir, before_2013_dir)\n",
    "print('Moved all surveys from before 2013 into seperate folder')\n",
    "sentinel_img_survey(img_dir, csv_dir, sentinel_done, urban_radius, rural_radius, MAX_CLOUD_PROBABILITY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f5812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
