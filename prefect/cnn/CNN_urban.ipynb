{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e1ff7e9",
   "metadata": {},
   "source": [
    "# CNN for Urban Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "527b84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import random\n",
    "import tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "#to dos:\n",
    "#generalize some code like pixel counts\n",
    "#create a preprocessing script and outsource methods\n",
    "#write different methods for normalization\n",
    "#write some quality control methods, e.g. pdf (probability density functions, outlyer detection - implement\n",
    "#into normalization methods)\n",
    "#more to come"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b7fc3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move Urban images to own directory\n",
    "def get_urban(urban_dir, orig_dir, channels):\n",
    "\n",
    "    img_list = os.listdir(orig_dir)\n",
    "    survey_name = os.path.basename(orig_dir)\n",
    "    for img_name in img_list:\n",
    "        if img_name.endswith('.tif'):\n",
    "            img_dir = os.path.join(orig_dir, img_name)\n",
    "            with rasterio.open(img_dir) as img:\n",
    "                if len(channels) == 0:\n",
    "                    array = img.read()\n",
    "                else:\n",
    "                    array = img.read(channels)\n",
    "            if array.shape[1]< 500:\n",
    "                #Name tif data survey name + cluster\n",
    "                img_survey_name = img_name.replace(img_name[:6], survey_name)\n",
    "                urb_img = os.path.join(urban_dir, img_survey_name)\n",
    "                os.rename(img_dir, urb_img)\n",
    "                \n",
    "    channel_size = array.shape[0]\n",
    "    \n",
    "    return channel_size\n",
    "\n",
    "#Extract from each zip-file (each survey) all images which belongs to urban category (less than 500x500 pixels) and\n",
    "#move them to own directory\n",
    "def get_urban_img(base, channels):\n",
    "    #Define folders\n",
    "    urban_dir = os.path.join(base, 'urban')\n",
    "    if not os.path.exists(urban_dir):\n",
    "        os.mkdir(urban_dir)\n",
    "\n",
    "    zip_files = os.listdir(base)\n",
    "    for zip_file in zip_files:\n",
    "        if zip_file.endswith('.zip'):\n",
    "            zip_dir = os.path.join(base, zip_file)\n",
    "            with ZipFile(zip_dir, 'r') as zipObj:\n",
    "                # Extract all the contents of zip file in current directory\n",
    "                zipObj.extractall(base)\n",
    "                img_dir_name = os.path.splitext(zip_file)[0]\n",
    "                img_dir = os.path.join(base, img_dir_name)\n",
    "                channel_size = get_urban(urban_dir, img_dir, channels)\n",
    "                shutil.rmtree(img_dir)\n",
    "\n",
    "    return urban_dir, channel_size\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9450511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe where the column source refers to the water source which was named the most\n",
    "def get_cluster_num_from_img_name(img):\n",
    "    cluster_num = None\n",
    "    #Remove .tif (BFGE71FL0000203.tif )\n",
    "    img = img[:img.rfind('.')]\n",
    "    \n",
    "    #Remove survey name (has always the length  8, BFGE71FL0000203)\n",
    "    cluster_0 = img[8:]\n",
    "    \n",
    "    #Get cluster number by finding first entry in remaining string which is not 0 as they typically have the form\n",
    "    #B0000203 (want 203)\n",
    "    num_start = None\n",
    "    for str_index in range(0, len(cluster_0)):\n",
    "        if cluster_0[str_index] != '0':\n",
    "            num_start = str_index\n",
    "            break\n",
    "    if num_start is None:\n",
    "        cluster_num = 0\n",
    "    else:\n",
    "        cluster_num = cluster_0[num_start:]\n",
    "    \n",
    "    return cluster_num\n",
    "    \n",
    "    \n",
    "def get_main_source_file(water_file):\n",
    "   \n",
    "\n",
    "    water_source = water_file.drop(labels=['ID','cluster', 'residence','year'], axis = 1)\n",
    "    water_source = water_source.fillna(0)\n",
    "    water_source=  water_source.idxmax(axis=1)\n",
    "    water_source.name = 'source'\n",
    "\n",
    "\n",
    "    df = pd.concat([water_file['ID'], water_file['cluster'], water_source], axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Create label dataframe where next to name (image name (Survey name+Cluster number)) the label in string format\n",
    "#and categorial format (number) is provided, but only for sentinel images which main source belongs to the main label\n",
    "#defined prior (which we want to classify); if they don't belong to one of those main labels thoses images are removed\n",
    "def get_labels_df_for_img(csv_file, urban_dir, main_labels):\n",
    "    \n",
    "    img_list = os.listdir(urban_dir)\n",
    "    water_file = pd.read_csv(csv_file)\n",
    "    label_list = []\n",
    "    column_names =  [\"name\", \"source\", \"label\"]\n",
    "    \n",
    "    water_file_max = get_main_source_file(water_file)\n",
    "    \n",
    "    for img in img_list:\n",
    "        for index, survey_name in enumerate(water_file_max['ID']): \n",
    "                survey_name = survey_name.replace('HR', 'GE', 1)\n",
    "                if survey_name in img:\n",
    "                    #Check if cluster is fitting; note +.tif to ensure that e.g. 1 is not true for e.g. AOGe....2104.tif\n",
    "                    cluster_img = int(get_cluster_num_from_img_name(img))\n",
    "                    cluster_df = int(water_file_max.loc[index]['cluster'])\n",
    "                    if cluster_img == cluster_df:\n",
    "                        #Check if label is part of the main labels (if not remove the image)\n",
    "                        source = water_file_max.loc[index]['source']\n",
    "                        if source in main_labels:\n",
    "                            img_label = [img, source, source]\n",
    "                            label_list.append(img_label)\n",
    "                        else:\n",
    "                            img_dir = os.path.join(urban_dir,img)\n",
    "                            os.remove(img_dir)\n",
    "                            \n",
    "    #Get data frame for label list                        \n",
    "    label_array = np.array(label_list)\n",
    "    label_df = pd.DataFrame(label_array, columns = column_names) \n",
    "    #Turn label column from string entries to categorical entries\n",
    "    label_df.label = pd.Categorical(pd.factorize(label_df.label)[0] + 1)\n",
    "\n",
    "    print(label_df)\n",
    "    \n",
    "    return label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcfeed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac447854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data set into training, validation, and testing; each subset gets its own folder\n",
    "\n",
    "#Get ratio between validation to test set to be able to split the remaining data set properly (prior: Split into\n",
    "#training set and remaining set)\n",
    "def ratio_val_to_test(val, test):\n",
    "    \n",
    "    total = val + test\n",
    "    one_perc = 100.00/total\n",
    "    val_ratio = one_perc * val*0.01\n",
    "    \n",
    "    return val_ratio\n",
    "\n",
    "def created_data_sets(urban_dir, split_size):\n",
    "\n",
    "    img_list = os.listdir(urban_dir)\n",
    "\n",
    "    #Create  validation, training und test folder\n",
    "\n",
    "    train_dir = os.path.join(urban_dir,'training')\n",
    "    if not os.path.exists(train_dir):\n",
    "        os.mkdir(train_dir)\n",
    "\n",
    "    val_dir = os.path.join(urban_dir,'validation')\n",
    "    if not os.path.exists(val_dir):\n",
    "        os.mkdir(val_dir)\n",
    "\n",
    "    test_dir = os.path.join(urban_dir,'test')\n",
    "    if not os.path.exists(test_dir):\n",
    "        os.mkdir(test_dir)\n",
    "\n",
    "    #Split into the data sets and move them to their respective folder\n",
    "    \n",
    "    X_train, X_rem = train_test_split(img_list, train_size= split_size[0])\n",
    "    \n",
    "    X_val, X_test = train_test_split(X_rem, train_size = ratio_val_to_test(split_size[1], split_size[2]))\n",
    "\n",
    "    for img in X_train:\n",
    "            img_dir = os.path.join(urban_dir, img)\n",
    "            train_img = os.path.join(train_dir, img)\n",
    "            os.rename(img_dir, train_img)\n",
    "\n",
    "    for img in X_val:\n",
    "            img_dir = os.path.join(urban_dir, img)\n",
    "            val_img = os.path.join(val_dir, img)\n",
    "            os.rename(img_dir, val_img)\n",
    "\n",
    "    for img in X_test:\n",
    "            img_dir = os.path.join(urban_dir, img)\n",
    "            test_img = os.path.join(test_dir, img)\n",
    "            os.rename(img_dir, test_img)\n",
    "    \n",
    "    \n",
    "    return train_dir, val_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08a55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative calculation way of mean and std if dataset is not too big\n",
    "def calc_mean_std(data_dir, input_height, input_width,clipping_values, channels, channel_size):\n",
    "    \n",
    "    img_list = os.listdir(data_dir)\n",
    "    assert all([i.endswith('.tif') for i in img_list])\n",
    "    \n",
    "    pixels = np.ndarray(shape(len(img_list), channel_size, input_height, input_width))\n",
    "    \n",
    "    #Create \"array of all images\"\n",
    "    for i, img_name in enumerate(img_list):\n",
    "        img_dir = os.path.join(data_dir, img_name)\n",
    "        with rasterio.open(img_dir) as img:\n",
    "            if len(channels) == 0:\n",
    "                array = img.read()\n",
    "            else:\n",
    "                array = img.read(channels)\n",
    "\n",
    "            \n",
    "        array = array.astype('float32')\n",
    "        #Clipping\n",
    "        array = np.clip(array,a_min = clipping_values[0], a_max = clipping_values[1])\n",
    "        #Ensure that that all arrays have the same size\n",
    "        array = array[:,:input_height,:input_width]\n",
    "        pixels[i] = array\n",
    "        \n",
    "    #Calculate Mean and Standard deviation along images (axis 0), width & heigth(axis:2,3) for each channel (axis:1)       \n",
    "    means = pixels.mean(axis=(0,2,3), dtype='float64')\n",
    "    stds = pixels.std(axis=(0,2,3), dtype='float64')\n",
    "\n",
    "    \n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e0200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate mean for each channel over all pixels for training set; for validation and test set you need to take\n",
    "#mean and std of training set as well as in real case scenarios you don't know them beforehand to calculate them\n",
    "def calc_mean(data_dir,input_height, input_width, clipping_values, channels):\n",
    "    \n",
    "    img_list = os.listdir(data_dir)\n",
    "    assert all([i.endswith('.tif') for i in img_list])\n",
    "    #pixels = np.ndarray(shape=(len(img_list), 13,201, 201))\n",
    "    #Variable to save the summation of the pixels values\n",
    "    sum_arr = 0\n",
    "    #Count of pixels\n",
    "    sum_pixel = input_height*input_width*len(img_list)\n",
    "\n",
    "    for i, img_name in enumerate(img_list):\n",
    "        img_dir = os.path.join(data_dir, img_name)\n",
    "        with rasterio.open(img_dir) as img:\n",
    "            if len(channels) == 0:\n",
    "                array = img.read()\n",
    "            else:\n",
    "                array = img.read(channels)\n",
    "\n",
    "        array = array.astype('float32')\n",
    "        array[np.isnan(array)] = 0\n",
    "        #Clipping\n",
    "        array = np.clip(array,a_min = clipping_values[0],a_max = clipping_values[1])\n",
    "        #Ensure that that all arrays have the same size\n",
    "        array = array[:,:input_height,:input_width]\n",
    "        #pixels[i] = array\n",
    "        sum_arr += array.sum(axis = (1,2))\n",
    "        \n",
    "    #Calculate mean\n",
    "    means = sum_arr/sum_pixel\n",
    "    \n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b85db78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate standard deviation (note:mean function has to be executed beforehand as it is required as input)\n",
    "def calc_std(means, data_dir, input_height, input_width, clipping_values, channels):\n",
    "    img_list = os.listdir(data_dir)\n",
    "    assert all([i.endswith('.tif') for i in img_list])\n",
    "    \n",
    "    sum_arr = 0\n",
    "    #Count of pixels\n",
    "    sum_pixel = input_height*input_width*len(img_list)\n",
    "    \n",
    "    #Calculate mean and take it ^2\n",
    "    for i, img_name in enumerate(img_list):\n",
    "        img_dir = os.path.join(data_dir, img_name)\n",
    "        with rasterio.open(img_dir) as img:\n",
    "            if len(channels) == 0:\n",
    "                array = img.read()\n",
    "            else:\n",
    "                array = img.read(channels)\n",
    "\n",
    "        array = array.astype('float32')\n",
    "        array[np.isnan(array)] = 0\n",
    "\n",
    "        #Clipping\n",
    "        array = np.clip(array,a_min = clipping_values[0],a_max =clipping_values[1])\n",
    "        #Ensure that that all arrays have the same size\n",
    "        array = array[:,:input_height,:input_width]\n",
    "    \n",
    "        array = np.power(array.transpose(1,2,0) - means, 2).transpose(2, 0, 1)\n",
    "        sum_arr += array.sum(axis = (1,2))\n",
    "    \n",
    "    stds = np.sqrt(sum_arr/sum_pixel)\n",
    "    \n",
    "    return stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639a27f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate iterable object for model.fit()\n",
    "def generator(x_dir, labels, batch_size, means, stds, input_height, input_width, clipping_values, channels, channel_size, num_labels):\n",
    "\n",
    "    x_list = os.listdir(x_dir)\n",
    "    assert all([i.endswith('.tif') for i in x_list])\n",
    "    #Shuffle elements in list, so that batches consists of images of different surveys\n",
    "    random.shuffle(x_list)\n",
    "    #generate batches (x : input, y: label)\n",
    "    batch_x = np.zeros(shape=(batch_size, channel_size,input_height, input_width))\n",
    "    batch_y = np.zeros(shape=(batch_size,num_labels), dtype=int)\n",
    "    #Iterator\n",
    "    batch_ele = 0\n",
    "\n",
    "    for x in x_list:\n",
    "        #Get training sample x\n",
    "        img_dir = os.path.join(x_dir, x)\n",
    "        \n",
    "        with rasterio.open(img_dir) as img:\n",
    "            if len(channels) == 0:\n",
    "                array = img.read().astype(\"float32\")\n",
    "            else:\n",
    "                array = img.read(channels).astype(\"float32\")\n",
    "\n",
    "        \n",
    "        array[np.isnan(array)] = 0\n",
    "        assert not np.any(np.isnan(array)), \"Float\"\n",
    "        #Clipping\n",
    "        array = np.clip(array,a_min = clipping_values[0],a_max = clipping_values[1])\n",
    "\n",
    "        assert not np.any(np.isnan(array)), \"After clipping\"\n",
    "        #Ensure that that all arrays have the same size via cropping\n",
    "        array = array[:,:input_height,:input_width]\n",
    "        \n",
    "        #Normalize the array\n",
    "        array = ((array.transpose(1,2,0)-means)/stds).transpose(2, 0, 1)\n",
    "        assert not np.any(np.isnan(array)), \"Normalize\"\n",
    "        # Add to batch\n",
    "        batch_x[batch_ele] = array     \n",
    "\n",
    "        #Get corresponding Label y: old version\n",
    "        '''for index, survey_name in enumerate(labels['ID']): \n",
    "            survey_name = survey_name.replace('HR', 'GE', 1)\n",
    "            if survey_name in x:\n",
    "                #find for this row in dataframe labels corresponding cluster and check with this if it is filename of image\n",
    "                cluster = labels.loc[index]['cluster']\n",
    "                #cluster solely not enough as e.g. 1 may also be in 100, 101, 110, ....\n",
    "                cluster_string = '000'+str(cluster)+'.tif'\n",
    "                if cluster_string in x:\n",
    "                    one_hot = np.zeros(shape = 3)\n",
    "                    label_pos = (labels.loc[index]['label'])-1\n",
    "                    print(labels.loc[index]['label'], label_pos)\n",
    "                    #One hot encoding\n",
    "                    one_hot[label_pos] = 1\n",
    "                    batch_y[batch_ele] = one_hot'''\n",
    "        \n",
    "        #Get corresponding label y\n",
    "        for index, survey_name in enumerate(labels['name']):\n",
    "            if survey_name in x:\n",
    "                one_hot = np.zeros(shape = num_labels)\n",
    "                #As indexing starts at 0 \n",
    "                label_pos = (labels.loc[index]['label'])-1\n",
    "                #One hot encoding\n",
    "                one_hot[label_pos] = 1\n",
    "                batch_y[batch_ele] = one_hot\n",
    "                \n",
    "        #Check if batch is already full (Note: Index in batch array is from 0...4 hence we need to add +1 to batch_ele)\n",
    "        if (batch_ele+1) == batch_size:\n",
    "            batch_x = batch_x.transpose(0,2,3,1)\n",
    "            #Return of batch_x,batch_y\n",
    "            yield batch_x,batch_y\n",
    "            #Reset settings -> Start of next batch generation\n",
    "            batch_ele = 0\n",
    "            batch_x = np.zeros(shape=(batch_size, channel_size,input_height, input_width))\n",
    "            batch_y = np.zeros(shape=(batch_size, num_labels), dtype=int)\n",
    "\n",
    "        else:\n",
    "            batch_ele += 1\n",
    "    \n",
    "    \n",
    "#    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f54c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#PARAMETERS\n",
    "base = '/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/NN/sentinel/'\n",
    "orig_dir = os.path.join(base, 'AOGE71FL')\n",
    "water_source_file = \"/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/SatelliteImage__GEE/correlation/SAV_Data/water-source/joined-surveys-2013-grouped.csv\"\n",
    "batch_size = 5\n",
    "# 3 Main label categories (which are kept)\n",
    "main_labels = ['piped', 'groundwater', 'bottled water']\n",
    "num_labels = len(main_labels)\n",
    "#Training, validation and test set size \n",
    "#[Training size, validation size, test size]\n",
    "split_size = [0.8,0.1,0.1]\n",
    "#Input height\n",
    "input_height  = 201\n",
    "#Input width \n",
    "input_width = 201\n",
    "#Minimum and maximum values (for clipping above and below those values)\n",
    "#[Minimum value, maximum value]\n",
    "clipping_values = [0, 3000]\n",
    "\n",
    "#channels (define channels which should be used, if all should list can stay empty channel = [])\n",
    "channels = [4,3,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6985ff2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "Moved urbane images to seperate folder\n"
     ]
    }
   ],
   "source": [
    "#Functions\n",
    "urban_dir, channel_size =  get_urban_img(base, channels)\n",
    "print('Moved urbane images to seperate folder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b9d8148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     name       source label\n",
      "0    AOGE71FL00000124.tif        piped     1\n",
      "1    AOGE71FL00000121.tif  groundwater     2\n",
      "2    AOGE71FL00000101.tif        piped     1\n",
      "3    AOGE71FL00000100.tif        piped     1\n",
      "4    AOGE71FL00000094.tif        piped     1\n",
      "..                    ...          ...   ...\n",
      "356  BFGE71FL00000164.tif        piped     1\n",
      "357  BFGE71FL00000163.tif        piped     1\n",
      "358  BFGE71FL00000162.tif        piped     1\n",
      "359  BFGE71FL00000149.tif  groundwater     2\n",
      "360  BFGE71FL00000144.tif        piped     1\n",
      "\n",
      "[361 rows x 3 columns]\n",
      "Add to label names categorical labels and removed images which don't belong to one of the main labels\n"
     ]
    }
   ],
   "source": [
    "urban_dir ='/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/NN/sentinel/urban'\n",
    "labels_df = get_labels_df_for_img(water_source_file, urban_dir, main_labels)\n",
    "print(\"Add to label names categorical labels and removed images which don't belong to one of the main labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c366469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split up data set into training, validation and test data\n"
     ]
    }
   ],
   "source": [
    "train_dir, val_dir, test_dir = created_data_sets(urban_dir, split_size)\n",
    "print('Split up data set into training, validation and test data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d0bf93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean and standard deviation for each channel (for training set)\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/NN/sentinel/urban/training'\n",
    "val_dir = '/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/NN/sentinel/urban/validation'\n",
    "means = calc_mean(train_dir, input_height, input_width, clipping_values, channels)\n",
    "stds = calc_std(means, train_dir, input_height, input_width, clipping_values, channels)\n",
    "print('Calculated mean and standard deviation for each channel (for training set)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43441768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created x and y for training data\n"
     ]
    }
   ],
   "source": [
    "training_generator = generator(train_dir, labels_df, batch_size, means, stds, input_height, input_width, clipping_values, channels, channel_size, num_labels)\n",
    "#Check if shape is correct\n",
    "print('Created x and y for training data')\n",
    "'''for data_batch, labels_batch in training_generator:\n",
    "    print('This is the shape of the training data batch:', data_batch.shape)\n",
    "    print('This is the shape of the training label batch:', labels_batch.shape)\n",
    "    break'''\n",
    "\n",
    "validation_generator = generator(val_dir, labels_df, batch_size, means, stds, input_height, input_width, clipping_values, channels, channel_size, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c365c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(16, (3, 3), activation='relu',input_shape=(201, 201, channel_size)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9cafe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 199, 199, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 99, 99, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 97, 97, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 21, 21, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               3277312   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 3,339,363\n",
      "Trainable params: 3,339,363\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f1bfeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 [==============================] - 1s 221ms/step - loss: 1.0097 - accuracy: 0.6106\n",
      "Epoch 2/10\n",
      "5/5 [==============================] - 1s 214ms/step - loss: 0.6038 - accuracy: 0.6678\n",
      "Epoch 3/10\n",
      "5/5 [==============================] - 1s 216ms/step - loss: 0.4970 - accuracy: 0.7672\n",
      "Epoch 4/10\n",
      "5/5 [==============================] - 1s 210ms/step - loss: 0.5965 - accuracy: 0.6956\n",
      "Epoch 5/10\n",
      "5/5 [==============================] - 1s 215ms/step - loss: 0.5476 - accuracy: 0.8067\n",
      "Epoch 6/10\n",
      "5/5 [==============================] - 1s 229ms/step - loss: 0.4309 - accuracy: 0.8306\n",
      "Epoch 7/10\n",
      "5/5 [==============================] - 1s 222ms/step - loss: 0.6714 - accuracy: 0.7089\n",
      "Epoch 8/10\n",
      "5/5 [==============================] - 1s 207ms/step - loss: 0.9873 - accuracy: 0.4744\n",
      "Epoch 9/10\n",
      "5/5 [==============================] - 1s 209ms/step - loss: 0.6111 - accuracy: 0.7633\n",
      "Epoch 10/10\n",
      "5/5 [==============================] - 1s 217ms/step - loss: 0.5917 - accuracy: 0.6511\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "history = model.fit(\n",
    "            training_generator,\n",
    "            steps_per_epoch=5,\n",
    "            epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOLLOWING PART IS JUST TRY OUT AND NOT FINISHED AT ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3f589e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RESNET Model\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "#import cv2\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a5e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc7128bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, filters, stage, block):\n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    # Save the input value. We'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "\n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Second component of main path\n",
    "    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    # Third component of main path\n",
    "    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
    "\n",
    "    # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3a7fa20",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43363/1865189499.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentity_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as test:\n",
    "    A_prev = tf.placeholder(\"float\", [3, 4, 4, 6])\n",
    "    X = np.random.randn(3, 4, 4, 6)\n",
    "    A = identity_block(A_prev, f = 2, filters = [2, 4, 6], stage = 1, block = 'a')\n",
    "    test.run(tf.global_variables_initializer())\n",
    "    out = test.run([A], feed_dict={A_prev: X, K.learning_phase(): 0})\n",
    "    print(\"out = \", out[0][1][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e702aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
