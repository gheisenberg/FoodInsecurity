{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This program extracts the relevant DHS-part dealing with water sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "from ethiopian_date import EthiopianDateConverter as edc\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional if ZIP Data with FL.zip ending was downloaded\n",
    "def remove_FL(dir_name:str):\n",
    "    #Remove all Zip-Data which doesn't include .sav\n",
    "    folder = os.listdir(dir_name)\n",
    "    for item in folder:\n",
    "        if item.endswith(\"FL.zip\") or item.endswith(\"FL.ZIP\"):\n",
    "            os.remove(os.path.join(dir_name, item))\n",
    "            \n",
    "# Optional if not only Houehold Surveys (HR) were downloaded\n",
    "def remove_all_except_HR(dir_name:str):\n",
    "    #Remove all Zip-Data which don't belong to a HR-survey (a HR survey is indicated by 'HR' on the 3-4 position\n",
    "    #of the survey name)\n",
    "\n",
    "    folder = os.listdir(dir_name)\n",
    "    for item in folder:\n",
    "        if not \"HR\" in item[2:4] and (item.endswith(\".zip\") or item.endswith(\".ZIP\")):\n",
    "            os.remove(os.path.join(dir_name, item))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sav(listOfFileNames:list, newpath:str, zipObject: ZipFile):\n",
    "    ZipFile\n",
    "    # Extract solely the sav-file from the zip and save them into a seperate folder\n",
    "    for fileName in listOfFileNames:\n",
    "        if fileName.endswith('.sav') or fileName.endswith('.SAV'):\n",
    "            # Extract the sav file from the zip\n",
    "            zipObject.extract(fileName, newpath)\n",
    "            \n",
    "            \n",
    "def extract_bigger_zip(zip_dir:str,filenames:str,newpath:str):\n",
    "#For the case if a zip folder includes zip folders: \n",
    "#Take zip folder to get sav within main zip folder which is the biggest on of all w.r.t. to size (bytes)           \n",
    "    \n",
    "    #Create temporary directory for this survey\n",
    "    zips_dir = zip_dir[:zip_dir.find('.')]\n",
    "    if not os.path.exists(zips_dir):\n",
    "        os.makedirs(zips_dir)\n",
    "        \n",
    "    #Extract the zip folders into the directory    \n",
    "    with ZipFile(zip_dir, 'r') as zipObj:\n",
    "        zipObj.extractall(zips_dir)\n",
    "\n",
    "    big_size = 0\n",
    "    big_zip = None\n",
    "    \n",
    "    #Compare zip folders and take the biggest one\n",
    "    for item in filenames:\n",
    "        if item.endswith('.zip') or item.endswith('.ZIP'):\n",
    "            file_dir = os.path.join(zips_dir, item)\n",
    "            curr_size= os.stat(file_dir).st_size\n",
    "            if curr_size >= big_size:\n",
    "                big_size = curr_size\n",
    "                big_zip = file_dir\n",
    "    \n",
    "    # Now check via check_sav if dbf-file is given or if you have again zip-files within the current zip file\n",
    "    check_sav(big_zip,newpath)    \n",
    "    \n",
    "#Second mainpart for single zip file        \n",
    "def check_sav(zip_dir:str, newpath:str):\n",
    "    \n",
    "    big_zip = None\n",
    "    #Get list of all elements within the zipfile\n",
    "    with ZipFile(zip_dir, 'r') as zipObject:\n",
    "        listOfFileNames = zipObject.namelist()\n",
    "        \n",
    "        #Check if any element within the zip is a sav file if yes jump into function get_sav\n",
    "        if any((element.endswith('.sav') or element.endswith('.SAV')) for element in listOfFileNames):\n",
    "            get_sav(listOfFileNames, newpath, zipObject)\n",
    "            \n",
    "        #If \"if-clause\" not true, check here if any element is a zip file if yes jump into extract_bigger_zip\n",
    "        elif any((element.endswith('.zip') or element.endswith('.ZIP')) for element in listOfFileNames):\n",
    "            extract_bigger_zip(zip_dir, listOfFileNames, newpath)  \n",
    "            \n",
    "def delete_zip_folders(dir_name:str,newpath:str):\n",
    "    \n",
    "    #Delete all directories which may be created during the process to figure out which zip folder is bigger             \n",
    "    list_subfolders_with_paths = [f.path for f in os.scandir(dir_name) if f.is_dir()]\n",
    "    for folder in list_subfolders_with_paths:\n",
    "        if not folder == newpath:\n",
    "            shutil.rmtree(folder)\n",
    "            \n",
    "#Main part-> runs through all zip files in a directory  \n",
    "def extract_sav(dir_name:str, newpath:str):\n",
    "    '''The HR Surveys retrieved (for SPSS) contain usually a sav-file from which\n",
    "    we can gather all relevant information; in this part we save those sav-files in a seperate folder'''\n",
    "    \n",
    "    #Create folder for SAV files\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "        \n",
    "    #List of all survey zips in the directory dir_name    \n",
    "    folder = os.listdir(dir_name)\n",
    "    \n",
    "    #Extract sav file (if existing) from each zip file via check_sav\n",
    "    for item in folder:\n",
    "        if item.endswith('.zip') or item.endswith('.ZIP'):\n",
    "            zip_dir = os.path.join(dir_name, item)\n",
    "            check_sav(zip_dir, newpath)\n",
    "    \n",
    "    #Delete all directories via the following function\n",
    "    delete_zip_folders(dir_name,newpath)\n",
    "    \n",
    "    return newpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv data with information of year, water source, and region type for each cluster\n",
    "\n",
    "# Find the label number for the question regarding water source\n",
    "def find_water_var(df: pd.DataFrame, meta_dict: pyreadstat._readstat_parser.metadata_container):\n",
    "    water = None\n",
    "    for i in meta_dict:\n",
    "        #Check if meta_dict is empty\n",
    "        if meta_dict is None or meta_dict[i] is None:\n",
    "            print('Water', meta_dict[i])\n",
    "        # If question is given in meta_dict[i] we take the label i of it (if it is the first one we find)\n",
    "        elif \"source of drinking water\" in meta_dict[i].lower():\n",
    "                if not df[i].isnull().all().all():\n",
    "                    if water == None:\n",
    "                        water = i\n",
    "                    else:\n",
    "                        print('Water...2nd possibility', i)\n",
    "    return water\n",
    "\n",
    "# Find the label number for the cluster number\n",
    "def find_cluster_var(df: pd.DataFrame, meta_dict: pyreadstat._readstat_parser.metadata_container):\n",
    "    cluster = None\n",
    "    for i in meta_dict:\n",
    "        if meta_dict is None or meta_dict[i] is None:\n",
    "            print(meta_dict[i])\n",
    "        # If question is given in meta_dict[i] we take the label i of it (if it is the first one we find)\n",
    "        elif \"Cluster number\" in meta_dict[i] or \"cluster number\" in meta_dict[i]:\n",
    "                        if not df[i].isnull().all().all():\n",
    "                            cluster = i\n",
    "\n",
    "    return cluster\n",
    "\n",
    "# Find the label number for the year the survey was conducted\n",
    "def find_year_var(df: pd.DataFrame, meta_dict: pyreadstat._readstat_parser.metadata_container):\n",
    "    year = None\n",
    "    for i in meta_dict:\n",
    "        if meta_dict is None or meta_dict[i] is None:\n",
    "            print(meta_dict[i])\n",
    "        # If question is given in meta_dict[i] we take the label i of it (if it is the first one we find)\n",
    "        elif \"Year of interview\" in meta_dict[i] or \"year of interview\" in meta_dict[i]:\n",
    "                    if not df[i].isnull().all().all():\n",
    "                        year = i\n",
    "    return year\n",
    "\n",
    "# Find the label number for the residence type\n",
    "def find_residence_var(df: pd.DataFrame, meta_dict: pyreadstat._readstat_parser.metadata_container):\n",
    "    residence = None\n",
    "    for i in meta_dict:\n",
    "        if meta_dict is None or meta_dict[i] is None:\n",
    "            print(meta_dict[i])\n",
    "        # If question is given in meta_dict[i] we take the label i of it (if it is the first one we find)\n",
    "        elif  \"Type of place of residence\" in meta_dict[i] or \"type of place of residence\" in meta_dict[i]:\n",
    "            if not df[i].isnull().all().all():\n",
    "                residence = i\n",
    "    return residence      \n",
    "\n",
    "#Changing from ethopian date in HR-Surveys(not needed for GE-Data) to Gregorian dates\n",
    "def get_eth_to_gregorian(df_year:pd.DataFrame, year_HV:str, len_row:int):\n",
    "    \n",
    "    #Translation of Ethopian year into the (two) Gregorian years as one Ethopian year spans over 2 Greg. years\n",
    "    #and return the list which contains the years values (num_year times each)\n",
    "    def gregorian_dates (year:int, num_year:int):\n",
    "        greg_list = list()\n",
    "        \n",
    "        year_first_half = edc.to_gregorian(int(year),1,1).year\n",
    "        year_second_half = None\n",
    "        for month in range(1,13):\n",
    "            # 5.day as the 13 Month has at usually only 5 days (except leapyear 6 days).\n",
    "            year_scnd = edc.to_gregorian(int(year), month, 5).year\n",
    "            if year_scnd != year_first_half:\n",
    "                year_second_half = year_scnd\n",
    "\n",
    "        greg_list.extend([year_first_half]*num_year)\n",
    "        greg_list.extend([year_second_half]*num_year)\n",
    "\n",
    "\n",
    "        return greg_list\n",
    "\n",
    "    def check_for_emtpy_year_rows(num_years:int, table_len:int, year:int):\n",
    "        diff = table_len-num_years\n",
    "        return [year]*diff\n",
    "\n",
    "    #Main\n",
    "    gregorian_list = list()\n",
    "    #Extract the earliest year\n",
    "    year_min = df_year[year_HV].min()\n",
    "    #Extract the latest year the survey was conducted\n",
    "    year_max = df_year[year_HV].max()\n",
    "\n",
    "    #For the case that the earliest and the latest year are not the same we need to translate both year into Gregorian\n",
    "    # the min year in Gregorian dates gets to replace half of the Ethopian dates; and max year in Gregorian dates gets to replace\n",
    "    #half of the ethopian date. A year in Ethopian year spans over two years in Gregorian dates, hence we need to \n",
    "    #split the number of Ethopian date values to replace for each Gregorian year by four (two for earliest Eth. date\n",
    "    #and two for latest Eth. date)\n",
    "    if year_max != year_min:\n",
    "        num_year = int(len_row/4)\n",
    "        #Add the  sub-year-list via gregorian_dates to the main year list gregorian_list\n",
    "        gregorian_list.extend(gregorian_dates(year_min, num_year))\n",
    "        gregorian_list.extend(gregorian_dates(year_max, num_year))\n",
    "        #Case that num_year == len(crosstab[year]) due to int-transformation, add to rows w/o values year_max)\n",
    "        if not len_row == num_year*4:\n",
    "            #Extend the year list gregorian_list so that the length of the list == #cluster\n",
    "            gregorian_list.extend(check_for_emtpy_year_rows(num_year*4,len_row, year_max))\n",
    "    \n",
    "    #If you have only one year in Ethopian year. The survey spans at most over two years in Gregorian date\n",
    "    else:\n",
    "        num_year = int(len_row/2)\n",
    "        gregorian_list.extend(gregorian_dates(year_max, num_year))\n",
    "        if not len_row == num_year*2:\n",
    "            gregorian_list.extend(check_for_emtpy_year_rows(num_year*2, len_row, year_max))\n",
    "\n",
    "    return gregorian_list\n",
    "\n",
    "def get_csv(file:str, export_path:str):\n",
    "    #Encoding of the sav file into dataframe df and meta file meta\n",
    "    df, meta = pyreadstat.read_sav(file, encoding = 'LATIN1')\n",
    "    #from meta file we extract the columns names and labels/ID\n",
    "    meta_dict = dict(zip(meta.column_names, meta.column_labels))\n",
    "    \n",
    "    cluster = None\n",
    "    water = None\n",
    "    year = None\n",
    "    residence = None\n",
    "    #The relevant columns names have ID such as HV201...those labels are for the same question (like \"Residence type\")\n",
    "    #usually identically for all surveys; but if not we jump into a function which searches for the label for the \n",
    "    #question we desire to include in our csv\n",
    "    \n",
    "    #Water source question has usually the label HV201 if not existing as label in meta_dict, jump into find_water_var\n",
    "    if 'HV201' in meta_dict.keys():\n",
    "        water = 'HV201'\n",
    "    else:\n",
    "        water = find_water_var(df, meta_dict)\n",
    "        print('Water',water,file[file.rfind('/'):])\n",
    "    #Cluster (number) question has usually the label HV001 if not existing as label in meta_dict, jump into find_cluster_var\n",
    "    if 'HV001' in meta_dict.keys():\n",
    "        cluster = 'HV001'\n",
    "    else:\n",
    "        cluster = find_cluster_var(df, meta_dict)\n",
    "        print('Cluster',cluster, file[file.rfind('/'):])\n",
    "    \n",
    "    #Year question has usually the label HV007 if not existing as label in meta_dict, jump into find_year_var\n",
    "    if 'HV007' in meta_dict.keys():\n",
    "        year = 'HV007'\n",
    "    else:\n",
    "        year = find_year_var(df, meta_dict)\n",
    "        print('Year',year, file[file.rfind('/'):])\n",
    "        \n",
    "    #Residence type question has usually the label HV025 if not existing as label in meta_dict, jump into find_residence_var\n",
    "    if 'HV025' in meta_dict.keys():\n",
    "        residence = 'HV025'\n",
    "    else:\n",
    "        residence = find_residence_var(df, meta_dict)\n",
    "        print('residence', residence, file[file.rfind('/'):])\n",
    "    \n",
    "    \n",
    "    try:      \n",
    "        #Construction cross tab for cluster (rows) & water sources (each water source has its own column)\n",
    "        crosstab = pd.crosstab(df[cluster], df[water].map(meta.variable_value_labels[water]),rownames = [\"cluster\"],colnames = [\"Properties\"], dropna=True)\n",
    "        #export = file[file.rfind('/'):file.rfind('.')]\n",
    "        #Get survey name (solely without path to it)\n",
    "        filename = os.path.basename(file[:file.rfind(\".\")])\n",
    "        #Add year column to the crosstab\n",
    "        \n",
    "        # In Ethopian HR Surveys the year is given based on the Ethopian calendar. Hence, a \"translation\" to\n",
    "        # Gregorian calendar is required\n",
    "        if filename.startswith('ETHR'):\n",
    "            #Translation into gregorian calendar\n",
    "            gregorian_list = get_eth_to_gregorian(df, year, len(crosstab))\n",
    "            #Adding year column gregorian_list to cross tab\n",
    "            crosstab.insert(loc = len(crosstab.columns), column = \"year\", value = gregorian_list)  \n",
    "        else:               \n",
    "            # Construct cross tab for cluster & year -> form: E.g. cluster: 24, 2014: 1, 2015: 0 [column name : entry]\n",
    "            table = pd.crosstab(df[cluster] , df[year], rownames = [\"cluster\"], colnames= [\"year\"], dropna=True)\n",
    "            #As table contains for each year mention in the survey an own column, we need to merge them to\n",
    "            #one single column where the column values are the years, hence idmax (so from e.g. 2014:0, 2015:1 for \n",
    "            #cluster 24  we geht year : 2015)\n",
    "            years = table.idxmax(axis=1)\n",
    "            #Add years to cross tab with column name 'year'\n",
    "            crosstab['year'] = years\n",
    "\n",
    "        #Add residence values (Rural, Urban, Refugee)\n",
    "        # Construct cross tab for cluster & residence-> form: E.g. cluster: 24, urban: 1, rural: 0, refugee:0 [column name : entry]\n",
    "        residence_tab = pd.crosstab(df[cluster], df[residence].map(meta.variable_value_labels[residence]), rownames = [\"Cluster\"], dropna=True)\n",
    "        #As table contains for each residence type mention in the survey an own column, we need to merge them to\n",
    "        #one single column where the column values are the residence type, hence idmax (so from e.g. urban: 1, rural: 0, refugee:0 for \n",
    "        #cluster 24  we geht residence: urban)\n",
    "        residences = residence_tab.idxmax(axis=1)\n",
    "        #Add residences to cross tab with column name residence\n",
    "        crosstab['residence'] = residences\n",
    "        \n",
    "        #Rename column name of the cluster column to cluster as it has been not named yet (Unnamed: 0)\n",
    "        crosstab.rename( columns={'Unnamed: 0':'cluster'}, inplace=True )\n",
    "        #Define the export path\n",
    "        export = os.path.join(export_path, filename+'-water_source.csv')\n",
    "        #Export the cross tab as csv-file\n",
    "        crosstab.to_csv(export)\n",
    "    except Exception as e:\n",
    "        print('Error', os.path.basename(file), e)\n",
    "\n",
    "def create_csv(dir_sav:str, dir_csv:str):\n",
    "    '''Create CSV files for each survey (if they have a dbf file); The information for the csv files are extracted\n",
    "    from the dbf file; the final csv file contains the following columns 'cluster' (number), 'year', 'residence' \n",
    "    (type) and the columsn regarding the water sources; for each cluster of the survey there is a seperate row'''\n",
    "\n",
    "    #Create the general Csv-directory where all csv files are stored\n",
    "    if not os.path.exists(dir_csv):\n",
    "        os.makedirs(dir_csv)\n",
    "    #List of all paths to the dbf files\n",
    "    directory = os.listdir(dir_sav)\n",
    "    for file in directory: \n",
    "        sav_path = os.path.join (dir_sav, file)\n",
    "        get_csv(sav_path, dir_csv)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''AS the Sentinel 2 data used as input for the Neuronal Network is only available since May 2015, we decided to \n",
    "only use surveys which were conducted from 2013-until now. Hence, this part moves all surveys conducted before\n",
    "2013 to a seperate folder. Note: As soon as one cluster has the year 2013 or later, we do not move them to\n",
    "the seperate folder'''\n",
    "\n",
    "def split_before_2013 (export_path:str):\n",
    "    \n",
    "    #Create seperate folder for the surveys conducted before 2013\n",
    "    before_2013 = os.path.join(export_path, \"before_2013\")\n",
    "    if not os.path.exists(before_2013):\n",
    "        os.makedirs(before_2013)\n",
    "    \n",
    "    #Check which files where conducted before 2013\n",
    "    directory = os.listdir(export_path)    \n",
    "    for file in directory:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_file = os.path.join(export_path, file)\n",
    "            survey_year = pd.read_csv(csv_file, usecols = ['year'])\n",
    "            #Check if the latest year is smaller than 2013. If yes, move them to the new directory\n",
    "            if survey_year['year'].max()< 2013:\n",
    "                new_path = os.path.join(before_2013, file)\n",
    "                os.rename(csv_file, new_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''As we use Sentinel Data as Input for the NN, we require the locations of the clusters for retrieving them.\n",
    "The location is given within the GeoData DHS files. But not all HR-Survey have corresponding Geodata files. Hence,\n",
    "all surveys which do not have GeoData file, are moved to seperate folder. For this step it is required that\n",
    "the program satellite_images_gee was already executed or at least the csv files for the GeoData were already\n",
    "created'''\n",
    "def check_year_country (gps_dir:str, water_file:str):\n",
    "    \n",
    "    dir_gps = os.listdir(gps_dir)\n",
    "    #Run through all GeoData files to check if one might correspond to your water file (HR file) w.r.t to year\n",
    "    #country. If yes, we return a False and don't move (in the main function into the seperate folder)\n",
    "    for gps_csv in dir_gps:\n",
    "        if gps_csv.endswith('.csv'):\n",
    "            gps_file = os.path.join(gps_dir, gps_csv)\n",
    "            #Read out only year column from current GeoData csv file\n",
    "            gps_years = pd.read_csv(gps_file, usecols = ['year'])\n",
    "            #Take a random value of them column (here of index 1) and use it as reference year for the GeoData\n",
    "            gps_year = gps_years.iloc[1]['year']\n",
    "            #Read out only year column from current GeoData csv file\n",
    "            water_years = pd.read_csv(water_file, usecols = ['year'])\n",
    "            #if gps_year['year'].equals(water_year['year']):\n",
    "            #Jump into if-clause if any the value given in the HR-Survey water column is equal to the GeoData year gps_year\n",
    "            if any(water_years.iloc[i]['year'] == gps_year for i in range(len(water_years))):\n",
    "                #Jump into If-Clause, if the number of clusters is the same\n",
    "                if len(gps_years) == len(water_years):\n",
    "                    #Get country abbreviation of GeoData and HR file (abbreviation corresponds to the first two\n",
    "                    #letter of the survey/file name)\n",
    "                    country_gps = os.path.basename(gps_file)[:2]\n",
    "                    country_water = os.path.basename(water_file)[:2]\n",
    "                    #Jump into the if-clause and return False, if the country is the same\n",
    "                    if country_gps == country_water:\n",
    "                        water = os.path.basename(water_file[:water_file.rfind('-')])\n",
    "                        gps = os.path.basename(gps_file[:gps_file.rfind(\".\")])\n",
    "                        print(water, gps)\n",
    "                        return False\n",
    "    return True\n",
    "\n",
    "#Main function of this part\n",
    "def split_no_gps(dir_csv:str, dir_no_gps:str, dir_gps_zips:str, dir_gps_csv:str):\n",
    "    \n",
    "    #Create directory for the Hr survey which do not have corresponding GeoData files\n",
    "    if not os.path.exists(dir_no_gps):\n",
    "        os.makedirs(dir_no_gps)\n",
    "    \n",
    "    water_dir = os.listdir(dir_csv)\n",
    "    gps_dir = os.listdir(dir_gps_zips)\n",
    "    gps_cvs_dir = os.listdir(dir_gps_csv)\n",
    "    \n",
    "    \n",
    "    for water_file in water_dir:\n",
    "        if water_file.endswith('.csv'):\n",
    "            #Some HR files and corresponding GeoData files have the same survey name except the abbrevation \n",
    "            # 'HR' and 'GE'. Hence, we at first check if the survey name is the same if 'HR' is replaced by\n",
    "            #'GE'. If not, we need to jump into the function check_year_country to decide if a corresponding\n",
    "            #Ge file exists. If check_year_country() is true we move this csv-file (HR survey) to the newly create folder.\n",
    "            possible_gps_name = water_file.replace('HR', 'GE')[:water_file.rfind('-')]\n",
    "            if not any(possible_gps_name in gps_file for gps_file in gps_cvs_dir):\n",
    "                current_path = os.path.join(dir_csv, water_file)\n",
    "                if check_year_country(dir_gps_csv, current_path):\n",
    "                    new_path = os.path.join(dir_no_gps, water_file)\n",
    "                    os.rename(current_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create from all single csv-files one big. This simplifies later one when creating the categorical labels for the \n",
    "#CNN the coding. Furthermore, it is easier to analyze the water sources and their namings in the surveys overall\n",
    "\n",
    "def create_single_csv(dir_csv:str, path:str):\n",
    "    \n",
    "    directory = os.listdir(dir_csv)    \n",
    "    big_csv = pd.DataFrame()\n",
    "    # Path to joined file (if already existing delete to avoid adding it in the for-loop to the csv data)\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    \n",
    "    for file in directory:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_file = os.path.join(dir_csv, file)\n",
    "            current_csv = pd.read_csv(csv_file)\n",
    "            #Add ID/survey name as column to current_csv file; name clip at -water_source.csv\n",
    "            filename = os.path.basename(file)[:file.find('-')]\n",
    "            ID = [filename]*len(current_csv)\n",
    "            idx = 0\n",
    "            current_csv.insert(loc=idx, column='ID', value = ID)\n",
    "            #Append the current df to final big df file\n",
    "            big_csv = pd.concat([big_csv, current_csv])\n",
    "            \n",
    "    #Export as csv file   \n",
    "    big_csv.to_csv(path,index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We manually joined columns as discussed with Sven & Yvonne (June 2021) to minimize the number of possible labels\n",
    "\n",
    "def merge_columns_big_csv(dir_big_csv:str, path:str):\n",
    "    # Path to joined file (if already existing delete to avoid adding it in the for-loop to the csv data)\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    \n",
    "    df = pd.read_csv(dir_big_csv)\n",
    "        \n",
    "    df_grouped = df['ID']\n",
    "    df_grouped = pd.Series.to_frame(df_grouped)\n",
    "    df_grouped['cluster'] = df['cluster']\n",
    "    df_grouped['residence'] = df['residence']\n",
    "    df_grouped['year'] = df['year']\n",
    "    df = df.fillna(0)\n",
    "    df_grouped['piped'] = df['Piped into dwelling'] + df['Piped to yard/plot']+df[\"Piped to neighbour's house\"]\\\n",
    "                            +df['Piped to neighbor'] + df['Public fountain'] + df['Public tap/standpipe']+\\\n",
    "                            +df['Piped from the neighbor']+df['Public to neighborhood']+df[\"Neighbour's tap\"]+\\\n",
    "                            +df[\"Neighbor's house\"]\n",
    "    df_grouped['groundwater'] = df['Tube well or borehole'] + df['Hand pump / Tube well or borehole']+\\\n",
    "                                df['Borehole in yard/plot'] +df['Public borehole'] +df['Borehole with pump']+\\\n",
    "                                df['Protected well'] +df['Unprotected well']+df['Protected spring']+\\\n",
    "                                df['Unprotected spring']\n",
    "    df_grouped['surface water'] = df['Lake/pond/river/channel/irrigation channel']+df['River/dam/lake/ponds/stream/canal/irrigation channel']+\\\n",
    "                                    df['Gravity flow scheme']\n",
    "    df_grouped['rain'] = df['Rainwater']\n",
    "    df_grouped['external source'] = df['Tanker truck'] + df['Cart with small tank'] + df['Vendor']+\\\n",
    "                                    df['Motorcycle with three wheels'] + df['Bicycle with jerrycans']\n",
    "    df_grouped['bottled water'] = df['Bottled water'] + df['Bottled water or sachets']+df['Water sachets']+\\\n",
    "                                    df['Bag water'] + df['Sachet water (in a bag)'] + df['Sachet'] +\\\n",
    "                                    df['Water in plastic bag'] + df['Water in sachet'] + df['Sachet water'] +\\\n",
    "                                    df['Mineral water in sachet']\n",
    "\n",
    "    df_grouped.to_csv(path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e75269f9add>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdir_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/SatelliteImage__GEE/correlation/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdir_zip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_corr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SAV_Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdir_sav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SAV_file'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdir_gps_zips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'GPS_Data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Main part \n",
    "\n",
    "#Paths\n",
    "dir_corr = '/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/SatelliteImage__GEE/correlation/'\n",
    "dir_zip = os.path.join(dir_corr,'SAV_Data')\n",
    "dir_sav = os.path.join(dir_zip, 'SAV_file')\n",
    "dir_gps_zips = os.path.join(dir_corr, 'GPS_Data')\n",
    "dir_gps_csv = os.path.join(dir_gps_zips, 'gps_csv')\n",
    "dir_csv =  os.path.join(dir_zip, 'water-source')\n",
    "dir_no_gps = os.path.join(dir_csv, 'no_GPS_from_2013')\n",
    "dir_joined_csv = os.path.join(dir_csv, 'joined-surveys-2013.csv')\n",
    "dir_joined_csv_merged_columns = os.path.join(dir_csv, 'joined-surveys-2013-grouped.csv')\n",
    "\n",
    "#Functions\n",
    "remove_FL(dir_zip)\n",
    "print('remove FL done')\n",
    "remove_all_except_HR(dir_zip)\n",
    "print('remove_all_except_HR done')\n",
    "extract_sav(dir_zip, dir_sav)\n",
    "print('created sav files')\n",
    "create_csv(dir_sav, dir_csv)\n",
    "print('Create water source csv files')\n",
    "split_before_2013(dir_csv)\n",
    "print('Split into two subsets (before and after 2013)')\n",
    "#PLEASE NOTE: For the next function the \n",
    "if os.path.isdir(dir_gps_csv):\n",
    "    split_no_gps(dir_csv, dir_no_gps, dir_gps_zips, dir_gps_csv)\n",
    "    print('Moved all files without gps data into seperate subfolder')\n",
    "else:\n",
    "    print('No GPS-CSV created yet and hence, water_csv cannot be classified into without or with GPS data')\n",
    "    \n",
    "create_single_csv(dir_csv, dir_joined_csv) \n",
    "print('Created single big csv')\n",
    "\n",
    "merge_columns_big_csv(dir_joined_csv, dir_joined_csv_merged_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HV001 Cluster number\n",
    "HV201 Water source\n",
    "HV007 Year of interview\n",
    "HV025 Residence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please Note that the column names are rather diverse (1) although they may indicate the same source (e.g. *River/dam/lake/ponds/stream/canal/irrigation channel* and *Lake/pond/river/channel/irrigation channel*) or (2) there are different categories used (e.g. UGHR7IFL-water_source has the category *Bicycle with jerrycans* which others don't have)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
