{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "from zipfile import ZipFile\n",
    "import shutil\n",
    "from ethiopian_date import EthiopianDateConverter as edc\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional if ZIP Data with FL.zip ending was downloaded\n",
    "def remove_FL(dir_name):\n",
    "    #Remove all Zip-Data which doesn't include .sav\n",
    "    folder = os.listdir(dir_name)\n",
    "    for item in folder:\n",
    "        if item.endswith(\"FL.zip\") or item.endswith(\"FL.ZIP\"):\n",
    "            os.remove(os.path.join(dir_name, item))\n",
    "            \n",
    "# Optional if not only Houehold Surveys (HR) were downloaded\n",
    "def remove_all_except_HR(dir_name):\n",
    "    folder = os.listdir(dir_name)\n",
    "    for item in folder:\n",
    "        if not \"HR\" in item[2:4] and (item.endswith(\".zip\") or item.endswith(\".ZIP\")):\n",
    "            os.remove(os.path.join(dir_name, item))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract solely the sav-file from the zip and save them into a seperate folder\n",
    "def get_sav(listOfFileNames, newpath, zipObject):\n",
    "\n",
    "    for fileName in listOfFileNames:\n",
    "        if fileName.endswith('.sav') or fileName.endswith('.SAV'):\n",
    "            # Extract a single file from zip\n",
    "            zipObject.extract(fileName, newpath)\n",
    "            \n",
    "            \n",
    "def extract_bigger_zip(zip_dir,filenames,newpath):\n",
    "    zips_dir = zip_dir[:zip_dir.find('.')]\n",
    "    if not os.path.exists(zips_dir):\n",
    "        os.makedirs(zips_dir)\n",
    "    with ZipFile(zip_dir, 'r') as zipObj:\n",
    "    #Extract all the contents of zip file in different directory\n",
    "        zipObj.extractall(zips_dir)\n",
    "\n",
    "    big_size = 0\n",
    "    big_zip = None\n",
    "    for item in filenames:\n",
    "        if item.endswith('.zip') or item.endswith('.ZIP'):\n",
    "            file_dir = os.path.join(zips_dir, item)\n",
    "            curr_size= os.stat(file_dir).st_size\n",
    "            if curr_size >= big_size:\n",
    "                big_size = curr_size\n",
    "                big_zip = file_dir\n",
    "    check_sav(big_zip,newpath)    \n",
    "    \n",
    "#Second mainpart for single zip file        \n",
    "def check_sav(zip_dir, newpath):\n",
    "    big_zip = None\n",
    "    with ZipFile(zip_dir, 'r') as zipObject:\n",
    "        listOfFileNames = zipObject.namelist()\n",
    "\n",
    "        if any((element.endswith('.sav') or element.endswith('.SAV')) for element in listOfFileNames):\n",
    "            get_sav(listOfFileNames, newpath, zipObject)\n",
    "        elif any((element.endswith('.zip') or element.endswith('.ZIP')) for element in listOfFileNames):\n",
    "            extract_bigger_zip(zip_dir, listOfFileNames, newpath)  \n",
    "            \n",
    "def delete_zip_folders(dir_name,newpath):\n",
    "    list_subfolders_with_paths = [f.path for f in os.scandir(dir_name) if f.is_dir()]\n",
    "    for folder in list_subfolders_with_paths:\n",
    "        if not folder == newpath:\n",
    "            shutil.rmtree(folder)\n",
    "#Main part-> runs through all zip files in directory  \n",
    "def extract_sav(dir_name, newpath):\n",
    "    #Create folder for SAV files\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    folder = os.listdir(dir_name)\n",
    "\n",
    "    for item in folder:\n",
    "        if item.endswith('.zip') or item.endswith('.ZIP'):\n",
    "            zip_dir = os.path.join(dir_name, item)\n",
    "            check_sav(zip_dir, newpath)\n",
    "            \n",
    "    delete_zip_folders(dir_name,newpath)\n",
    "    \n",
    "    return newpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv data with information of year, water source, and region type for each cluster\n",
    "\n",
    "def find_water_var(df, meta_dict):\n",
    "    water = None\n",
    "    for i in meta_dict:\n",
    "        if meta_dict is None or meta_dict[i] is None:\n",
    "            print('Water', meta_dict[i])\n",
    "        elif \"source of drinking water\" in meta_dict[i].lower():\n",
    "                if not df[i].isnull().all().all():\n",
    "                    if water == None:\n",
    "                        water = i\n",
    "                    else:\n",
    "                        print('Water...2nd possibility', i)\n",
    "    return water\n",
    "def find_cluster_var(df, meta_dict):\n",
    "    cluster = None\n",
    "    for i in meta_dict:\n",
    "        if meta_dict is None or meta_dict[i] is None:\n",
    "            print(meta_dict[i])\n",
    "        elif \"Cluster number\" in meta_dict[i] or \"cluster number\" in meta_dict[i]:\n",
    "                        if not df[i].isnull().all().all():\n",
    "                            cluster = i\n",
    "\n",
    "    return cluster\n",
    "\n",
    "def find_year_var(df, meta_dict):\n",
    "    year = None\n",
    "    for i in meta_dict:\n",
    "        if meta_dict is None or meta_dict[i] is None:\n",
    "            print(meta_dict[i])\n",
    "        elif \"Year of interview\" in meta_dict[i] or \"year of interview\" in meta_dict[i]:\n",
    "                    if not df[i].isnull().all().all():\n",
    "                        year = i\n",
    "    return year\n",
    "\n",
    "def find_residence_var(df, meta_dict):\n",
    "    residence = None\n",
    "    for i in meta_dict:\n",
    "        if meta_dict is None or meta_dict[i] is None:\n",
    "            print(meta_dict[i])\n",
    "        elif  \"Type of place of residence\" in meta_dict[i] or \"type of place of residence\" in meta_dict[i]:\n",
    "            if not df[i].isnull().all().all():\n",
    "                residence = i\n",
    "    return residence      \n",
    "\n",
    "#Changing from ethopian date in HR-Surveys(not GE-Data) to gregorian dates\n",
    "def get_eth_to_gregorian(df_year, year_HV, len_row):\n",
    "    \n",
    "    def gregorian_dates (year, num_year):\n",
    "        greg_list = list()\n",
    "\n",
    "        year_first_half = edc.to_gregorian(int(year),1,1).year\n",
    "        year_second_half = None\n",
    "        for month in range(1,13):\n",
    "            # 5.day as the 13 Month has at usually only 5 days (except leapyear 6 days).\n",
    "            year_scnd = edc.to_gregorian(int(year), month, 5).year\n",
    "            if year_scnd != year_first_half:\n",
    "                year_second_half = year_scnd\n",
    "\n",
    "        greg_list.extend([year_first_half]*num_year)\n",
    "        greg_list.extend([year_second_half]*num_year)\n",
    "\n",
    "\n",
    "        return greg_list\n",
    "\n",
    "    def check_for_emtpy_year_rows(num_years, table_len, year):\n",
    "        diff = table_len-num_years\n",
    "        return [year]*diff\n",
    "\n",
    "    #Main\n",
    "    gregorian_list = list()\n",
    "    year_min = df_year[year_HV].min()\n",
    "    year_max = df_year[year_HV].max()\n",
    "\n",
    "    if year_max != year_min:\n",
    "        num_year = int(len_row/4)\n",
    "        gregorian_list.extend(gregorian_dates(year_min, num_year))\n",
    "        gregorian_list.extend(gregorian_dates(year_max, num_year))\n",
    "        #Case that num_year == len(crosstab[year] due to int-transformation, add to rows w/o values year_max)\n",
    "        if not len_row == num_year*4:\n",
    "            gregorian_list.extend(check_for_emtpy_year_rows(num_year*4,len_row, year_max))\n",
    "    else:\n",
    "        num_year = int(len_row/2)\n",
    "        gregorian_list.extend(gregorian_dates(year_max, num_year))\n",
    "        if not len_row == num_year*2:\n",
    "            gregorian_list.extend(check_for_emtpy_year_rows(num_year*2, len_row, year_max))\n",
    "\n",
    "    return gregorian_list\n",
    "\n",
    "def get_csv(file, export_path):\n",
    "    df, meta = pyreadstat.read_sav(file, encoding = 'LATIN1')\n",
    "    meta_dict = dict(zip(meta.column_names, meta.column_labels))\n",
    "    cluster = None\n",
    "    water = None\n",
    "    year = None\n",
    "    residence = None\n",
    "    if 'HV201' in meta_dict.keys():\n",
    "        water = 'HV201'\n",
    "    else:\n",
    "        water = find_water_var(df, meta_dict)\n",
    "        print('Water',water,file[file.rfind('/'):])\n",
    "    \n",
    "    if 'HV001' in meta_dict.keys():\n",
    "        cluster = 'HV001'\n",
    "    else:\n",
    "        cluster = find_cluster_var(df, meta_dict)\n",
    "        print('Cluster',cluster, file[file.rfind('/'):])\n",
    "\n",
    "    if 'HV007' in meta_dict.keys():\n",
    "        year = 'HV007'\n",
    "    else:\n",
    "        year = find_year_var(df, meta_dict)\n",
    "        print('Year',year, file[file.rfind('/'):])\n",
    "\n",
    "    if 'HV025' in meta_dict.keys():\n",
    "        residence = 'HV025'\n",
    "    else:\n",
    "        residence = find_residence_var(df, meta_dict)\n",
    "        print('residence', residence, file[file.rfind('/'):])\n",
    "        \n",
    "    try:      \n",
    "        crosstab = pd.crosstab(df[cluster], df[water].map(meta.variable_value_labels[water]),rownames = [\"cluster\"],colnames = [\"Properties\"], dropna=True)\n",
    "        export = file[file.rfind('/'):file.rfind('.')]\n",
    "        filename = os.path.basename(file[:file.rfind(\".\")])\n",
    "        #Add years\n",
    "        if filename.startswith('ETHR'):\n",
    "            gregorian_list = get_eth_to_gregorian(df, year, len(crosstab))\n",
    "            crosstab.insert(loc = len(crosstab.columns), column = \"year\", value = gregorian_list)  \n",
    "        else:               \n",
    "            table = pd.crosstab(df[cluster] , df[year], rownames = [\"cluster\"], colnames= [\"year\"], dropna=True)\n",
    "            years = table.idxmax(axis=1)\n",
    "            crosstab['year'] = years\n",
    "\n",
    "        #Add residence values (Rural, Urban)   \n",
    "        residence_tab = pd.crosstab(df[cluster] , df[residence].map(meta.variable_value_labels[residence]), rownames = [\"Cluster\"], dropna=True)\n",
    "        residences = residence_tab.idxmax(axis=1)\n",
    "        crosstab['residence'] = residences\n",
    "        \n",
    "        crosstab.rename( columns={'Unnamed: 0':'cluster'}, inplace=True )\n",
    "        export = os.path.join(export_path, filename+'-water_source.csv')\n",
    "        crosstab.to_csv(export)\n",
    "    except Exception as e:\n",
    "        print('Error', os.path.basename(file), e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(dir_sav, dir_csv):\n",
    "    if not os.path.exists(dir_csv):\n",
    "        os.makedirs(dir_csv)\n",
    "\n",
    "    directory = os.listdir(dir_sav)\n",
    "    for file in directory: \n",
    "        #print(\"This is the file\", file)\n",
    "        sav_path = os.path.join (dir_sav, file)\n",
    "        get_csv(sav_path, dir_csv)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_before_2013 (export_path):\n",
    "    before_2013 = os.path.join(export_path, \"before_2013\")\n",
    "    if not os.path.exists(before_2013):\n",
    "        os.makedirs(before_2013)\n",
    "\n",
    "    directory = os.listdir(export_path)    \n",
    "    for file in directory:\n",
    "        #print(file)\n",
    "        if file.endswith('.csv'):\n",
    "            csv_file = os.path.join(export_path, file)\n",
    "            survey_year = pd.read_csv(csv_file, usecols = ['year'])\n",
    "\n",
    "            if survey_year['year'].max()< 2013:\n",
    "                new_path = os.path.join(before_2013, file)\n",
    "                os.rename(csv_file, new_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_year_country (gps_dir, water_file):\n",
    "    dir_gps = os.listdir(gps_dir)\n",
    "     \n",
    "    for gps_csv in dir_gps:\n",
    "        if gps_csv.endswith('.csv'):\n",
    "            gps_file = os.path.join(gps_dir, gps_csv)\n",
    "            gps_years = pd.read_csv(gps_file, usecols = ['year'])\n",
    "            gps_year = gps_years.iloc[1]['year']\n",
    "            water_years = pd.read_csv(water_file, usecols = ['year'])\n",
    "            #if gps_year['year'].equals(water_year['year']):\n",
    "            if any(water_years.iloc[i]['year'] == gps_year for i in range(len(water_years))):\n",
    "                if len(gps_years) == len(water_years):\n",
    "                    country_gps = os.path.basename(gps_file)[:2]\n",
    "                    country_water = os.path.basename(water_file)[:2]\n",
    "                    if country_gps == country_water:\n",
    "                        water = os.path.basename(water_file[:water_file.rfind('-')])\n",
    "                        gps = os.path.basename(gps_file[:gps_file.rfind(\".\")])\n",
    "                        print(water, gps)\n",
    "                        return False\n",
    "    return True\n",
    "\n",
    "def split_no_gps(dir_csv, dir_no_gps, dir_gps_zips, dir_gps_csv):\n",
    "    if not os.path.exists(dir_no_gps):\n",
    "        os.makedirs(dir_no_gps)\n",
    "    \n",
    "    water_dir = os.listdir(dir_csv)\n",
    "    gps_dir = os.listdir(dir_gps_zips)\n",
    "    gps_cvs_dir = os.listdir(dir_gps_csv)\n",
    "    \n",
    "    for water_file in water_dir:\n",
    "        if water_file.endswith('.csv'):\n",
    "            possible_gps_name = water_file.replace('HR', 'GE')[:water_file.rfind('-')]\n",
    "            if not any(possible_gps_name in gps_file for gps_file in gps_cvs_dir):\n",
    "                current_path = os.path.join(dir_csv, water_file)\n",
    "                if check_year_country(dir_gps_csv, current_path):\n",
    "                    new_path = os.path.join(dir_no_gps, water_file)\n",
    "                    os.rename(current_path, new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_csv(dir_csv):\n",
    "    directory = os.listdir(dir_csv)    \n",
    "    big_csv = pd.DataFrame()\n",
    "    # Path to joined file (if already existing delete to avoid adding it in the for-loop to the csv data)\n",
    "    path = os.path.join(dir_csv, 'joined-surveys-after-2003.csv')\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    \n",
    "    for file in directory:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_file = os.path.join(dir_csv, file)\n",
    "            current_csv = pd.read_csv(csv_file)\n",
    "            #Add ID as column to current_csv file; name clip at -water_source.csv\n",
    "            filename = os.path.basename(file)[:file.find('-')]\n",
    "            ID = [filename]*len(current_csv)\n",
    "            idx = 0\n",
    "            current_csv.insert(loc=idx, column='ID', value = ID)\n",
    "            #Append it to big csv file\n",
    "            big_csv = pd.concat([big_csv, current_csv])\n",
    "       \n",
    "    big_csv.to_csv(path,index = False)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL_done\n",
      "remove_all_except_HR done\n",
      "created sav files\n",
      "Error NGHR21FL.SAV 'HV201'\n",
      "Water...2nd possibility sh100\n",
      "Water hv201 /UGHR6AFL.SAV\n",
      "Cluster hv001 /UGHR6AFL.SAV\n",
      "Year hv007 /UGHR6AFL.SAV\n",
      "residence hv025 /UGHR6AFL.SAV\n",
      "Error JOHR21FL.SAV 'HV201'\n",
      "Create water source csv files\n",
      "Split into two subsets (before and after 2013)\n",
      "GHHR7BFL GHGE7AFL\n",
      "GHHR72FL GHGE71FL\n",
      "GHHR82FL GHGE81FL\n",
      "KEHR72FL KEGE71FL\n",
      "MLHR72FL MLGE71FL\n",
      "MWHR72FL MWGE71FL\n",
      "NGHR7AFL NGGE7BFL\n",
      "RWHR70FL RWGE72FL\n",
      "SLHR72FL SLGE71FL\n",
      "SNHR7HFL SNGE7AFL\n",
      "SNHR8BFL SNGE8AFL\n",
      "TGHR61FL TGGE62FL\n",
      "TZHR7BFL TZGE7AFL\n",
      "UGHR7BFL UGGE7AFL\n",
      "UGHR72FL UGGE71FL\n",
      "JOHR73FL JOGE71FL\n",
      "Moved all files without gps data into seperate subfolder\n"
     ]
    }
   ],
   "source": [
    "# Main part \n",
    "dir_corr = '/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/SatelliteImage__GEE/correlation/'\n",
    "dir_zip = os.path.join(dir_corr,'SAV_Data')\n",
    "dir_sav = os.path.join(dir_zip, 'SAV_file')\n",
    "dir_gps_zips = os.path.join(dir_corr, 'GPS_Data')\n",
    "dir_gps_csv = os.path.join(dir_gps_zips, 'gps_csv')\n",
    "dir_csv =  os.path.join(dir_zip, 'water-source')\n",
    "dir_no_gps = os.path.join(dir_csv, 'no_GPS_from_2013')\n",
    "\n",
    "remove_FL(dir_zip)\n",
    "print('FL_done')\n",
    "remove_all_except_HR(dir_zip)\n",
    "print('remove_all_except_HR done')\n",
    "#extract_sav(dir_zip, dir_sav)\n",
    "print('created sav files')\n",
    "create_csv(dir_sav, dir_csv)\n",
    "print('Create water source csv files')\n",
    "split_before_2013(dir_csv)\n",
    "print('Split into two subsets (before and after 2013)')\n",
    "#PLEASE NOTE: For the next function the \n",
    "if os.path.isdir(dir_gps_csv):\n",
    "    split_no_gps(dir_csv, dir_no_gps, dir_gps_zips, dir_gps_csv)\n",
    "    print('Moved all files without gps data into seperate subfolder')\n",
    "else:\n",
    "    print('No GPS-CSV created yet and hence, water_csv cannot be classified into without or with GPS data')\n",
    "    \n",
    "big_csv_path = create_single_csv(dir_csv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please Note that the column names are rather diverse (1) although they may indicate the same source (e.g. *River/dam/lake/ponds/stream/canal/irrigation channel* and *Lake/pond/river/channel/irrigation channel*) or (2) there are different categories used (e.g. UGHR7IFL-water_source has the category *Bicycle with jerrycans* which others don't have)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HV001 Cluster number\n",
    "HV201 Water source\n",
    "HV007 Year of interview\n",
    "HV025 Residence\n",
    "\n",
    "PCR -> PCOM Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
