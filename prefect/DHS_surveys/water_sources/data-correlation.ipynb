{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "from zipfile import ZipFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional if ZIP Data with FL.zip ending was downloaded\n",
    "def remove_FL(dir_name):\n",
    "    #Remove all Zip-Data which doesn't include .sav\n",
    "    folder = os.listdir(dir_name)\n",
    "    for item in folder:\n",
    "        if item.endswith(\"FL.zip\") or item.endswith(\"FL.ZIP\"):\n",
    "            os.remove(os.path.join(dir_name, item))\n",
    "            \n",
    "# Optional if not only Houehold Surveys (HR) were downloaded\n",
    "def remove_all_except_HR(dir_name):\n",
    "    folder = os.listdir(dir_name)\n",
    "    for item in folder:\n",
    "        if not \"HR\" in item[2:4] and (item.endswith(\".zip\") or item.endswith(\".ZIP\")):\n",
    "            os.remove(os.path.join(dir_name, item))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract solely the sav-file from the zip and save them into a seperate folder\n",
    "def extract_sav(dir_name):\n",
    "    #Create folder for SAV files\n",
    "    newpath =  os.path.join(dir_name,'SAV_file')\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "\n",
    "    folder = os.listdir(dir_name)\n",
    "\n",
    "    for item in folder:\n",
    "        if item.endswith('.zip') or item.endswith('.ZIP'):\n",
    "            with ZipFile(dir_name+'/'+item, 'r') as zipObject:\n",
    "                listOfFileNames = zipObject.namelist()\n",
    "                for fileName in listOfFileNames:\n",
    "                    if fileName.endswith('.sav') or fileName.endswith('.SAV'):\n",
    "                        # Extract a single file from zip\n",
    "                        zipObject.extract(fileName, newpath)\n",
    "                        \n",
    "    return newpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create csv data with information of year, water source, and region type for each cluster\n",
    "def get_csv(file, export_path):\n",
    "    df, meta = pyreadstat.read_sav(file, encoding = 'LATIN1')\n",
    "    meta_dict = dict(zip(meta.column_names, meta.column_labels))\n",
    "    cluster = None\n",
    "    water = None\n",
    "    year = None\n",
    "    residence = None\n",
    "    for i in meta_dict:\n",
    "        if meta_dict is None or meta_dict[i] is None:\n",
    "            print(meta_dict[i])\n",
    "        else:\n",
    "            if \"Source of drinking water\" in meta_dict[i]:\n",
    "                if not df[i].isnull().all().all():\n",
    "                    water = i\n",
    "            elif \"Cluster number\" in meta_dict[i] or \"cluster number\" in meta_dict[i]:\n",
    "                if not df[i].isnull().all().all():\n",
    "                    cluster = i\n",
    "            elif \"Year of interview\" in meta_dict[i] or \"year of interview\" in meta_dict[i]:\n",
    "                    if not df[i].isnull().all().all():\n",
    "                        year = i\n",
    "            elif \"Type of place of residence\" in meta_dict[i] or \"type of place of residence\" in meta_dict[i]:\n",
    "                    if not df[i].isnull().all().all():\n",
    "                        residence = i\n",
    "    # V113 Source of drinking water, V115 Time to get to water source\n",
    "    #V001 Cluster number\n",
    "    #year of interview\n",
    "    #Type of place of residence\n",
    "    \n",
    "    if cluster is not None and water is not None:\n",
    "        #print('Want to create CSV')\n",
    "        try:\n",
    "            crosstab = pd.crosstab(df[cluster], df[water].map(meta.variable_value_labels[water]),rownames = [\"Cluster\"],colnames = [\"Properties\"], dropna=True, normalize='columns')\n",
    "            crosstab['Year'] = df[year]\n",
    "            crosstab['Residence'] = df[residence].map(meta.variable_value_labels[residence])\n",
    "            export = file[file.rfind('/'):file.rfind('.')]\n",
    "            #print(export)\n",
    "            crosstab.to_csv(export_path+export+'-water_source.csv')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(dir_sav, dir_csv):\n",
    "    if not os.path.exists(dir_csv):\n",
    "        os.makedirs(dir_csv)\n",
    "\n",
    "    directory = os.listdir(dir_sav)\n",
    "    for file in directory: \n",
    "        #print(\"This is the file\", file)\n",
    "        sav_path = os.path.join (dir_sav, file)\n",
    "        get_csv(sav_path, dir_csv)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_before_2013 (export_path):\n",
    "    before_2013 = os.path.join(export_path, \"before_2013\")\n",
    "    if not os.path.exists(before_2013):\n",
    "        os.makedirs(before_2013)\n",
    "\n",
    "    directory = os.listdir(export_path)    \n",
    "    for file in directory:\n",
    "        #print(file)\n",
    "        if file.endswith('.csv'):\n",
    "            csv_file = os.path.join(export_path, file)\n",
    "            survey_year = pd.read_csv(csv_file, usecols = ['Year'])\n",
    "            if survey_year['Year'].max()< 2013:\n",
    "                new_path = os.path.join(before_2013, file)\n",
    "                os.rename(csv_file, new_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_single_csv(dir_csv):\n",
    "    directory = os.listdir(dir_csv)    \n",
    "    big_csv = pd.DataFrame()\n",
    "    # Path to joined file (if already existing delete to avoid adding it in the for-loop to the csv data)\n",
    "    path = os.path.join(dir_csv, 'joined-surveys-after-2003.csv')\n",
    "    if os.path.exists(path):\n",
    "        os.remove(path)\n",
    "    \n",
    "    for file in directory:\n",
    "        if file.endswith('.csv'):\n",
    "            csv_file = os.path.join(dir_csv, file)\n",
    "            current_csv = pd.read_csv(csv_file)\n",
    "            #Add ID as column to current_csv file; name clip at -water_source.csv\n",
    "            filename = os.path.basename(file)[:file.find('-')]\n",
    "            ID = [filename]*len(current_csv)\n",
    "            idx = 0\n",
    "            current_csv.insert(loc=idx, column='ID', value = ID)\n",
    "            #Append it to big csv file\n",
    "            big_csv = pd.concat([big_csv, current_csv])\n",
    "       \n",
    "    big_csv.to_csv(path,index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FL_done\n",
      "remove_all_except_HR done\n",
      "created sav files\n",
      "Create water source csv files\n",
      "Splitted into two subsets (before and after 2013)\n"
     ]
    }
   ],
   "source": [
    "# Main part \n",
    "dir_zip = '/home/shannon/Dokumente/Dokumente/studium/ASA/Projekt/SatelliteImage__GEE/correlation/SAV_Data'\n",
    "dir_csv =  os.path.join(dir_zip, 'water-source')\n",
    "\n",
    "remove_FL(dir_zip)\n",
    "print('FL_done')\n",
    "remove_all_except_HR(dir_zip)\n",
    "print('remove_all_except_HR done')\n",
    "dir_sav_file = extract_sav(dir_zip)\n",
    "print('created sav files')\n",
    "create_csv(dir_sav_file, dir_csv)\n",
    "print('Create water source csv files')\n",
    "split_before_2013(dir_csv)\n",
    "print('Splitted into two subsets (before and after 2013)')\n",
    "create_single_csv(dir_csv) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please Note that the column names are rather diverse (1) although they may indicate the same source (e.g. *River/dam/lake/ponds/stream/canal/irrigation channel* and *Lake/pond/river/channel/irrigation channel*) or (2) there are different categories used (e.g. UGHR7IFL-water_source has the category *Bicycle with jerrycans* which others don't have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
